{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Python-Data-Analysis\" data-toc-modified-id=\"Python-Data-Analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Python Data Analysis</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Libraries-and-dependencies\" data-toc-modified-id=\"Libraries-and-dependencies-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Libraries and dependencies</a></span></li><li><span><a href=\"#Import-packages-and-create-modules\" data-toc-modified-id=\"Import-packages-and-create-modules-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Import packages and create modules</a></span></li></ul></li><li><span><a href=\"#Data-Import/Export\" data-toc-modified-id=\"Data-Import/Export-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Import/Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Calibration-Data\" data-toc-modified-id=\"Load-Calibration-Data-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Load Calibration Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#AlphaSense-sensors\" data-toc-modified-id=\"AlphaSense-sensors-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>AlphaSense sensors</a></span></li></ul></li><li><span><a href=\"#Import-Local-CSV\" data-toc-modified-id=\"Import-Local-CSV-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Import Local CSV</a></span></li><li><span><a href=\"#Import-Test\" data-toc-modified-id=\"Import-Test-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Import Test</a></span></li><li><span><a href=\"#Import-from-API\" data-toc-modified-id=\"Import-from-API-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Import from API</a></span></li><li><span><a href=\"#Data-Export\" data-toc-modified-id=\"Data-Export-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Data Export</a></span></li></ul></li></ul></li><li><span><a href=\"#Formulas-/-Calculator\" data-toc-modified-id=\"Formulas-/-Calculator-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Formulas / Calculator</a></span></li><li><span><a href=\"#AlphaSense-Baseline-Calibration\" data-toc-modified-id=\"AlphaSense-Baseline-Calibration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>AlphaSense Baseline Calibration</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#TODO:-Correction-Checks\" data-toc-modified-id=\"TODO:-Correction-Checks-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>TODO: Correction Checks</a></span></li></ul></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Series-Plots\" data-toc-modified-id=\"Time-Series-Plots-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Time Series Plots</a></span></li><li><span><a href=\"#TODO:-Basic-Sensor-Correlations\" data-toc-modified-id=\"TODO:-Basic-Sensor-Correlations-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>TODO: Basic Sensor Correlations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Full-Seaborn-Correlogram\" data-toc-modified-id=\"Full-Seaborn-Correlogram-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Full Seaborn Correlogram</a></span></li><li><span><a href=\"#Basic-Seaborn-XYPlot\" data-toc-modified-id=\"Basic-Seaborn-XYPlot-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Basic Seaborn XYPlot</a></span></li></ul></li><li><span><a href=\"#TODO:-Anomaly-Detection\" data-toc-modified-id=\"TODO:-Anomaly-Detection-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>TODO: Anomaly Detection</a></span></li><li><span><a href=\"#Data-Model\" data-toc-modified-id=\"Data-Model-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Data Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-Functionality\" data-toc-modified-id=\"Basic-Functionality-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Basic Functionality</a></span></li><li><span><a href=\"#Feature-selection-and-data-training-split\" data-toc-modified-id=\"Feature-selection-and-data-training-split-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Feature selection and data training split</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO-Select-variables-here\" data-toc-modified-id=\"TODO-Select-variables-here-4.4.2.1\"><span class=\"toc-item-num\">4.4.2.1&nbsp;&nbsp;</span>TODO Select variables here</a></span></li><li><span><a href=\"#TODO-Preliminary-Checks\" data-toc-modified-id=\"TODO-Preliminary-Checks-4.4.2.2\"><span class=\"toc-item-num\">4.4.2.2&nbsp;&nbsp;</span>TODO Preliminary Checks</a></span></li></ul></li><li><span><a href=\"#TODO-Naive-Linear-Regression\" data-toc-modified-id=\"TODO-Naive-Linear-Regression-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>TODO Naive Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ordinary-Linear-Regression\" data-toc-modified-id=\"Ordinary-Linear-Regression-4.4.3.1\"><span class=\"toc-item-num\">4.4.3.1&nbsp;&nbsp;</span>Ordinary Linear Regression</a></span></li><li><span><a href=\"#Ordinary-Linear-Regression-with-differentiation\" data-toc-modified-id=\"Ordinary-Linear-Regression-with-differentiation-4.4.3.2\"><span class=\"toc-item-num\">4.4.3.2&nbsp;&nbsp;</span>Ordinary Linear Regression with differentiation</a></span></li></ul></li><li><span><a href=\"#TODO-ARIMA(X)-model\" data-toc-modified-id=\"TODO-ARIMA(X)-model-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>TODO ARIMA(X) model</a></span></li></ul></li></ul></li><li><span><a href=\"#R-Framework\" data-toc-modified-id=\"R-Framework-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>R Framework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialise-environment\" data-toc-modified-id=\"Initialise-environment-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Initialise environment</a></span></li><li><span><a href=\"#Install-dependencies\" data-toc-modified-id=\"Install-dependencies-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Install dependencies</a></span></li><li><span><a href=\"#Load-in-R-libraries\" data-toc-modified-id=\"Load-in-R-libraries-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Load in R libraries</a></span></li><li><span><a href=\"#Export-Data-to-R-Dataframe\" data-toc-modified-id=\"Export-Data-to-R-Dataframe-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Export Data to R Dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Renaming-and-timestamp-reading\" data-toc-modified-id=\"Renaming-and-timestamp-reading-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Renaming and timestamp reading</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pairs-Plot\" data-toc-modified-id=\"Pairs-Plot-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Pairs Plot</a></span></li><li><span><a href=\"#Coplot\" data-toc-modified-id=\"Coplot-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>Coplot</a></span></li></ul></li><li><span><a href=\"#Model-Iterations\" data-toc-modified-id=\"Model-Iterations-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Model Iterations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Diagnostics\" data-toc-modified-id=\"Model-Diagnostics-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>Model Diagnostics</a></span></li><li><span><a href=\"#Model-Fit-Plot\" data-toc-modified-id=\"Model-Fit-Plot-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>Model Fit Plot</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Traditional-R\" data-toc-modified-id=\"Using-Traditional-R-5.6.2.1\"><span class=\"toc-item-num\">5.6.2.1&nbsp;&nbsp;</span>Using Traditional R</a></span></li><li><span><a href=\"#Using-interactive-Plot\" data-toc-modified-id=\"Using-interactive-Plot-5.6.2.2\"><span class=\"toc-item-num\">5.6.2.2&nbsp;&nbsp;</span>Using interactive Plot</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    '''\n",
    "    <script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "        if (code_show){\n",
    "            $('div.input').hide();\n",
    "        } else {\n",
    "            $('div.input').show();\n",
    "        }\n",
    "        code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    \n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! pip install pytz==2017.2 fileupload==0.1.2 ipywidgets==6.0.0 pandas==0.20.1 numpy==1.12.1 matplotlib==2.0.2 seaborn==0.8.0\n",
    "# ! jupyter nbextension install --py fileupload \n",
    "# ! jupyter nbextension enable --py fileupload\n",
    "# ! jupyter nbextension install --py widgetsnbextension \n",
    "# ! jupyter nbextension enable --py widgetsnbextension\n",
    "# ! jupyter nbextension install https://rawgit.com/jfbercher/small_nbextensions/master/toc2.zip  --user\n",
    "# #! jupyter nbextension install --py toc2/main \n",
    "# ! jupyter nbextension enable toc2/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jupyter stuff\n",
    "#from IPython.display import display, Markdown, FileLink, FileLinks, clear_output, HTML\n",
    "#from IPython.core.display import HTML\n",
    "#from IPython.display import display, clear_output\n",
    "#import ipywidgets as widgets\n",
    "#from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "#\n",
    "## System\n",
    "#import io, pytz, os, time, datetime, fileupload\n",
    "#from shutil import copyfile\n",
    "#from os.path import dirname, join\n",
    "#\n",
    "## Plots\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#\n",
    "#sns.set(color_codes=True)\n",
    "#%matplotlib inline\n",
    "#matplotlib.style.use('seaborn-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Import/Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Calibration Data\n",
    "\n",
    "Sensor calibration data is stored in json files under the [smartcitizen-iscape_data repository](https://github.com/fablabbcn/smartcitizen-iscape-data/tree/internal_dev/calData) and is loaded automatically from the cells below.\n",
    "\n",
    "#### AlphaSense sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aux Zero Current</th>\n",
       "      <th>Sensitivity 1</th>\n",
       "      <th>Sensitivity 2</th>\n",
       "      <th>Serial No</th>\n",
       "      <th>Target 1</th>\n",
       "      <th>Target 2</th>\n",
       "      <th>Zero Current</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serial No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162031254</th>\n",
       "      <td>-20.80</td>\n",
       "      <td>568.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162031254</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-34.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162031257</th>\n",
       "      <td>-18.60</td>\n",
       "      <td>493.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162031257</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-69.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162031256</th>\n",
       "      <td>-13.90</td>\n",
       "      <td>601.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162031256</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-68.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581706</th>\n",
       "      <td>-35.30</td>\n",
       "      <td>581.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581706</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-72.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581707</th>\n",
       "      <td>-46.30</td>\n",
       "      <td>605.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581707</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-56.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581708</th>\n",
       "      <td>-51.10</td>\n",
       "      <td>597.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581708</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-61.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581709</th>\n",
       "      <td>-43.20</td>\n",
       "      <td>577.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581709</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-66.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581710</th>\n",
       "      <td>-33.40</td>\n",
       "      <td>570.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581710</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-82.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581711</th>\n",
       "      <td>-36.30</td>\n",
       "      <td>564.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581711</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-73.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581712</th>\n",
       "      <td>-71.90</td>\n",
       "      <td>598.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581712</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-90.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581713</th>\n",
       "      <td>-46.70</td>\n",
       "      <td>605.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581713</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-94.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581714</th>\n",
       "      <td>-52.00</td>\n",
       "      <td>600.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581714</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-80.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581715</th>\n",
       "      <td>-24.00</td>\n",
       "      <td>591.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581715</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-69.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581716</th>\n",
       "      <td>-7.60</td>\n",
       "      <td>615.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581716</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-67.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581717</th>\n",
       "      <td>-51.70</td>\n",
       "      <td>564.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581717</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-75.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581718</th>\n",
       "      <td>-21.10</td>\n",
       "      <td>564.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581718</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-63.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581720</th>\n",
       "      <td>-36.90</td>\n",
       "      <td>589.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581720</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581721</th>\n",
       "      <td>-19.20</td>\n",
       "      <td>588.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581721</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-68.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581722</th>\n",
       "      <td>-53.30</td>\n",
       "      <td>582.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581722</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-73.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581723</th>\n",
       "      <td>-30.00</td>\n",
       "      <td>606.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581723</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-78.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581724</th>\n",
       "      <td>-29.30</td>\n",
       "      <td>621.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581724</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-79.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581725</th>\n",
       "      <td>-35.60</td>\n",
       "      <td>577.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581725</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-73.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162581726</th>\n",
       "      <td>-30.90</td>\n",
       "      <td>564.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162581726</td>\n",
       "      <td>CO</td>\n",
       "      <td>na</td>\n",
       "      <td>-67.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202560427</th>\n",
       "      <td>15.40</td>\n",
       "      <td>-384.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202560427</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>25.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202560428</th>\n",
       "      <td>14.20</td>\n",
       "      <td>-385.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202560428</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202560430</th>\n",
       "      <td>17.70</td>\n",
       "      <td>-383.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202560430</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>31.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160413</th>\n",
       "      <td>11.30</td>\n",
       "      <td>-341.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160413</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>22.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160404</th>\n",
       "      <td>9.80</td>\n",
       "      <td>-354.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160404</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>26.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160405</th>\n",
       "      <td>8.50</td>\n",
       "      <td>-361.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160405</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>10.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160406</th>\n",
       "      <td>13.60</td>\n",
       "      <td>-362.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160406</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>19.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160417</th>\n",
       "      <td>7.60</td>\n",
       "      <td>-333.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160417</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160418</th>\n",
       "      <td>11.30</td>\n",
       "      <td>-347.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160418</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>16.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160419</th>\n",
       "      <td>-7.60</td>\n",
       "      <td>-326.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160419</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>19.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160420</th>\n",
       "      <td>10.10</td>\n",
       "      <td>-360.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160420</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>15.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160421</th>\n",
       "      <td>6.60</td>\n",
       "      <td>-358.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160421</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>26.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160422</th>\n",
       "      <td>10.70</td>\n",
       "      <td>-356.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160422</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>10.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202160423</th>\n",
       "      <td>4.70</td>\n",
       "      <td>-347.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202160423</td>\n",
       "      <td>NO2</td>\n",
       "      <td>na</td>\n",
       "      <td>18.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204560315</th>\n",
       "      <td>18.92</td>\n",
       "      <td>-421.58</td>\n",
       "      <td>-471.65</td>\n",
       "      <td>204560315</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>23.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204560316</th>\n",
       "      <td>19.86</td>\n",
       "      <td>-433.12</td>\n",
       "      <td>-466.29</td>\n",
       "      <td>204560316</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>23.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204560314</th>\n",
       "      <td>14.50</td>\n",
       "      <td>-446.36</td>\n",
       "      <td>-506.96</td>\n",
       "      <td>204560314</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>23.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160142</th>\n",
       "      <td>27.43</td>\n",
       "      <td>-445.44</td>\n",
       "      <td>-499.71</td>\n",
       "      <td>204160142</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>18.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160143</th>\n",
       "      <td>27.74</td>\n",
       "      <td>-482.50</td>\n",
       "      <td>-524.30</td>\n",
       "      <td>204160143</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>33.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160144</th>\n",
       "      <td>19.86</td>\n",
       "      <td>-464.08</td>\n",
       "      <td>-602.96</td>\n",
       "      <td>204160144</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>27.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160146</th>\n",
       "      <td>26.17</td>\n",
       "      <td>-455.28</td>\n",
       "      <td>-524.46</td>\n",
       "      <td>204160146</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>34.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160147</th>\n",
       "      <td>22.38</td>\n",
       "      <td>-443.58</td>\n",
       "      <td>-556.46</td>\n",
       "      <td>204160147</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>21.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160148</th>\n",
       "      <td>22.07</td>\n",
       "      <td>-438.50</td>\n",
       "      <td>-558.35</td>\n",
       "      <td>204160148</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>27.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160149</th>\n",
       "      <td>15.76</td>\n",
       "      <td>-488.09</td>\n",
       "      <td>-527.30</td>\n",
       "      <td>204160149</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>-1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160150</th>\n",
       "      <td>24.91</td>\n",
       "      <td>-455.83</td>\n",
       "      <td>-497.82</td>\n",
       "      <td>204160150</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>28.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160151</th>\n",
       "      <td>23.65</td>\n",
       "      <td>-481.37</td>\n",
       "      <td>-530.92</td>\n",
       "      <td>204160151</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160152</th>\n",
       "      <td>30.27</td>\n",
       "      <td>-420.44</td>\n",
       "      <td>-569.38</td>\n",
       "      <td>204160152</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>28.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160153</th>\n",
       "      <td>26.17</td>\n",
       "      <td>-459.43</td>\n",
       "      <td>-613.68</td>\n",
       "      <td>204160153</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>29.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160154</th>\n",
       "      <td>23.01</td>\n",
       "      <td>-470.04</td>\n",
       "      <td>-555.04</td>\n",
       "      <td>204160154</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>18.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160155</th>\n",
       "      <td>19.23</td>\n",
       "      <td>-414.98</td>\n",
       "      <td>-516.10</td>\n",
       "      <td>204160155</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>15.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160156</th>\n",
       "      <td>22.70</td>\n",
       "      <td>-410.40</td>\n",
       "      <td>-536.44</td>\n",
       "      <td>204160156</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>5.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160157</th>\n",
       "      <td>21.44</td>\n",
       "      <td>-489.69</td>\n",
       "      <td>-528.87</td>\n",
       "      <td>204160157</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>29.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160158</th>\n",
       "      <td>13.87</td>\n",
       "      <td>-489.16</td>\n",
       "      <td>-539.28</td>\n",
       "      <td>204160158</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>23.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160159</th>\n",
       "      <td>21.12</td>\n",
       "      <td>-473.09</td>\n",
       "      <td>-556.77</td>\n",
       "      <td>204160159</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>19.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160160</th>\n",
       "      <td>23.96</td>\n",
       "      <td>-423.82</td>\n",
       "      <td>-523.99</td>\n",
       "      <td>204160160</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>27.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160162</th>\n",
       "      <td>18.92</td>\n",
       "      <td>-470.36</td>\n",
       "      <td>-554.25</td>\n",
       "      <td>204160162</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>19.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160163</th>\n",
       "      <td>20.49</td>\n",
       "      <td>-459.95</td>\n",
       "      <td>-583.41</td>\n",
       "      <td>204160163</td>\n",
       "      <td>O3</td>\n",
       "      <td>NO2</td>\n",
       "      <td>29.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Aux Zero Current  Sensitivity 1  Sensitivity 2  Serial No Target 1  \\\n",
       "Serial No                                                                       \n",
       "162031254            -20.80         568.30           0.00  162031254       CO   \n",
       "162031257            -18.60         493.10           0.00  162031257       CO   \n",
       "162031256            -13.90         601.90           0.00  162031256       CO   \n",
       "162581706            -35.30         581.40           0.00  162581706       CO   \n",
       "162581707            -46.30         605.00           0.00  162581707       CO   \n",
       "162581708            -51.10         597.40           0.00  162581708       CO   \n",
       "162581709            -43.20         577.90           0.00  162581709       CO   \n",
       "162581710            -33.40         570.00           0.00  162581710       CO   \n",
       "162581711            -36.30         564.30           0.00  162581711       CO   \n",
       "162581712            -71.90         598.40           0.00  162581712       CO   \n",
       "162581713            -46.70         605.50           0.00  162581713       CO   \n",
       "162581714            -52.00         600.10           0.00  162581714       CO   \n",
       "162581715            -24.00         591.60           0.00  162581715       CO   \n",
       "162581716             -7.60         615.10           0.00  162581716       CO   \n",
       "162581717            -51.70         564.30           0.00  162581717       CO   \n",
       "162581718            -21.10         564.80           0.00  162581718       CO   \n",
       "162581720            -36.90         589.70           0.00  162581720       CO   \n",
       "162581721            -19.20         588.50           0.00  162581721       CO   \n",
       "162581722            -53.30         582.50           0.00  162581722       CO   \n",
       "162581723            -30.00         606.40           0.00  162581723       CO   \n",
       "162581724            -29.30         621.60           0.00  162581724       CO   \n",
       "162581725            -35.60         577.30           0.00  162581725       CO   \n",
       "162581726            -30.90         564.00           0.00  162581726       CO   \n",
       "202560427             15.40        -384.90           0.00  202560427      NO2   \n",
       "202560428             14.20        -385.90           0.00  202560428      NO2   \n",
       "202560430             17.70        -383.70           0.00  202560430      NO2   \n",
       "202160413             11.30        -341.60           0.00  202160413      NO2   \n",
       "202160404              9.80        -354.70           0.00  202160404      NO2   \n",
       "202160405              8.50        -361.10           0.00  202160405      NO2   \n",
       "202160406             13.60        -362.60           0.00  202160406      NO2   \n",
       "...                     ...            ...            ...        ...      ...   \n",
       "202160417              7.60        -333.90           0.00  202160417      NO2   \n",
       "202160418             11.30        -347.70           0.00  202160418      NO2   \n",
       "202160419             -7.60        -326.20           0.00  202160419      NO2   \n",
       "202160420             10.10        -360.20           0.00  202160420      NO2   \n",
       "202160421              6.60        -358.20           0.00  202160421      NO2   \n",
       "202160422             10.70        -356.90           0.00  202160422      NO2   \n",
       "202160423              4.70        -347.30           0.00  202160423      NO2   \n",
       "204560315             18.92        -421.58        -471.65  204560315       O3   \n",
       "204560316             19.86        -433.12        -466.29  204560316       O3   \n",
       "204560314             14.50        -446.36        -506.96  204560314       O3   \n",
       "204160142             27.43        -445.44        -499.71  204160142       O3   \n",
       "204160143             27.74        -482.50        -524.30  204160143       O3   \n",
       "204160144             19.86        -464.08        -602.96  204160144       O3   \n",
       "204160146             26.17        -455.28        -524.46  204160146       O3   \n",
       "204160147             22.38        -443.58        -556.46  204160147       O3   \n",
       "204160148             22.07        -438.50        -558.35  204160148       O3   \n",
       "204160149             15.76        -488.09        -527.30  204160149       O3   \n",
       "204160150             24.91        -455.83        -497.82  204160150       O3   \n",
       "204160151             23.65        -481.37        -530.92  204160151       O3   \n",
       "204160152             30.27        -420.44        -569.38  204160152       O3   \n",
       "204160153             26.17        -459.43        -613.68  204160153       O3   \n",
       "204160154             23.01        -470.04        -555.04  204160154       O3   \n",
       "204160155             19.23        -414.98        -516.10  204160155       O3   \n",
       "204160156             22.70        -410.40        -536.44  204160156       O3   \n",
       "204160157             21.44        -489.69        -528.87  204160157       O3   \n",
       "204160158             13.87        -489.16        -539.28  204160158       O3   \n",
       "204160159             21.12        -473.09        -556.77  204160159       O3   \n",
       "204160160             23.96        -423.82        -523.99  204160160       O3   \n",
       "204160162             18.92        -470.36        -554.25  204160162       O3   \n",
       "204160163             20.49        -459.95        -583.41  204160163       O3   \n",
       "\n",
       "          Target 2  Zero Current  \n",
       "Serial No                         \n",
       "162031254       na        -34.00  \n",
       "162031257       na        -69.40  \n",
       "162031256       na        -68.10  \n",
       "162581706       na        -72.80  \n",
       "162581707       na        -56.70  \n",
       "162581708       na        -61.50  \n",
       "162581709       na        -66.80  \n",
       "162581710       na        -82.90  \n",
       "162581711       na        -73.10  \n",
       "162581712       na        -90.20  \n",
       "162581713       na        -94.90  \n",
       "162581714       na        -80.40  \n",
       "162581715       na        -69.70  \n",
       "162581716       na        -67.20  \n",
       "162581717       na        -75.70  \n",
       "162581718       na        -63.70  \n",
       "162581720       na        -99.00  \n",
       "162581721       na        -68.40  \n",
       "162581722       na        -73.10  \n",
       "162581723       na        -78.80  \n",
       "162581724       na        -79.10  \n",
       "162581725       na        -73.10  \n",
       "162581726       na        -67.20  \n",
       "202560427       na         25.90  \n",
       "202560428       na         24.00  \n",
       "202560430       na         31.50  \n",
       "202160413       na         22.40  \n",
       "202160404       na         26.20  \n",
       "202160405       na         10.70  \n",
       "202160406       na         19.90  \n",
       "...            ...           ...  \n",
       "202160417       na         15.10  \n",
       "202160418       na         16.70  \n",
       "202160419       na         19.50  \n",
       "202160420       na         15.40  \n",
       "202160421       na         26.20  \n",
       "202160422       na         10.70  \n",
       "202160423       na         18.60  \n",
       "204560315      NO2         23.65  \n",
       "204560316      NO2         23.33  \n",
       "204560314      NO2         23.01  \n",
       "204160142      NO2         18.92  \n",
       "204160143      NO2         33.10  \n",
       "204160144      NO2         27.11  \n",
       "204160146      NO2         34.05  \n",
       "204160147      NO2         21.75  \n",
       "204160148      NO2         27.74  \n",
       "204160149      NO2         -1.58  \n",
       "204160150      NO2         28.06  \n",
       "204160151      NO2          8.83  \n",
       "204160152      NO2         28.69  \n",
       "204160153      NO2         29.95  \n",
       "204160154      NO2         18.60  \n",
       "204160155      NO2         15.13  \n",
       "204160156      NO2          5.99  \n",
       "204160157      NO2         29.64  \n",
       "204160158      NO2         23.96  \n",
       "204160159      NO2         19.55  \n",
       "204160160      NO2         27.43  \n",
       "204160162      NO2         19.55  \n",
       "204160163      NO2         29.95  \n",
       "\n",
       "[69 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cal_utils import alphasense\n",
    "\n",
    "alpha_calData = alphasense()\n",
    "\n",
    "display(alpha_calData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Local CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csv_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Test\n",
    "\n",
    "Import test from local test database. \n",
    "\n",
    "**Requirement**:\n",
    "\n",
    "- Include where the directory of your tests is (GIT LFS directory)\n",
    "- Make sure that the desired test is available and has been created with the yaml tool\n",
    "\n",
    "**The cell below will**:\n",
    "\n",
    "- Load all the kits within the test\n",
    "- Check if there were alphasense sensors and retrieve their calibration data and order\n",
    "- Check if there was a reference and convert it units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "\tLoading test 2018-07_INT_TEMP_CALIB_CASE_BOTH_25degC for performed from 2018-07-21 to 2017-07-21\n",
      "\tTest performed with commit 0451aad9735d20c2a03f8e47b39796baba098e08\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<hr />\n",
       "<p><strong>Comment:</strong> \n",
       "Temperature calibration test</p>\n",
       "<p>Reading Interval: 2s\n",
       "ON/OFF Sequence for mics: every 30min\n",
       "Number of sequences: 5\n",
       "ESP_ON time = 5s\n",
       "ESP_OFF time = 30s</p>\n",
       "<p>BOTH SHT31 and SCK in a case</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-----KIT-----\n",
      "\t\t-----CURRENT-----\n",
      "\t\t\tKit location Europe/Madrid\n",
      "\t\tKit CURRENT has been loaded\n",
      "\t\t-----TEMPERATURE-----\n",
      "\t\t\tKit location Europe/Madrid\n"
     ]
    },
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fdafd7763771>\u001b[0m in \u001b[0;36mloadButton\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadButton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mreadings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mreadings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclearButton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macoscar/Documents/04_Projects/02_FabLab/02_SmartCitizen/04_iScape/99_DataAnalysis/smartcitizen-iscape-data/notebooks/test_utils.pyc\u001b[0m in \u001b[0;36mloadTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeGrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'Unnamed'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Create dictionary and add it to the readings key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macoscar/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3959\u001b[0m         versionadded=''))\n\u001b[1;32m   3960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3961\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3963\u001b[0m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macoscar/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3392\u001b[0m         \u001b[0m_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3393\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3395\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macoscar/anaconda2/lib/python2.7/site-packages/pandas/core/base.pyc\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_cython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# caller can react\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macoscar/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_groupby_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'numeric_only'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macoscar/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only)\u001b[0m\n\u001b[1;32m   3279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3280\u001b[0m         new_items, new_blocks = self._cython_agg_blocks(\n\u001b[0;32m-> 3281\u001b[0;31m             how, alt=alt, numeric_only=numeric_only)\n\u001b[0m\u001b[1;32m   3282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macoscar/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[0;34m(self, how, alt, numeric_only)\u001b[0m\n\u001b[1;32m   3348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3350\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No numeric types to aggregate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3352\u001b[0m         \u001b[0;31m# reset the locs in the blocks to correspond to our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "from test_utils import *\n",
    "\n",
    "#testsDir = os.getcwd()\n",
    "testsDir = '/Users/macoscar/Documents/04_Projects/02_FabLab/02_SmartCitizen/04_iScape/03_Development/03_TestResults/TestStructure'\n",
    "                \n",
    "tests = getTests(testsDir)\n",
    "interact(selectTests,\n",
    "         x = widgets.SelectMultiple(options=tests, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTests, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "def loadButton(b):\n",
    "    global readings\n",
    "    readings = loadTest()\n",
    "\n",
    "def clearButton(b):\n",
    "    global readings\n",
    "    clear_output()\n",
    "    clearTests()\n",
    "    readings = {}\n",
    "\n",
    "loadB = widgets.Button(description='Load Tests')\n",
    "loadB.on_click(loadButton)\n",
    "\n",
    "resetB = widgets.Button(description='Clear Tests')\n",
    "resetB.on_click(clearButton)\n",
    "\n",
    "buttonBox = widgets.HBox([loadB, resetB])\n",
    "\n",
    "display(buttonBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_utils import *\n",
    "\n",
    "readingsAPI = {}\n",
    "\n",
    "devices = (4501, 4571, 4564, 4576)\n",
    "[getKitID(device, True) for device in devices]\n",
    "\n",
    "readings = getReadingsAPI(devices)\n",
    "\n",
    "readings['devices'][4501].keys()\n",
    "readings['devices'][4576]['valid']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global selected\n",
    "selected = []\n",
    "\n",
    "def selectedFilesChannels(x):\n",
    "    selected = list(x)\n",
    "    \n",
    "def exportFile(b):\n",
    "    for i in range(len(selected)):\n",
    "        b.f = selected[i]\n",
    "        exportDir = exportPath.value\n",
    "        if not os.path.exists(exportDir): os.mkdir(exportDir)\n",
    "        savePath = os.path.join(exportDir, b.f)\n",
    "        if not os.path.exists(savePath):\n",
    "            readings[b.f].to_csv(savePath, sep=\",\")\n",
    "            display(FileLink(savePath))\n",
    "        else:\n",
    "            display(widgets.HTML(' File Already exists!'))\n",
    "\n",
    "display(widgets.HTML('<h3>Export Files</h3>'))\n",
    "exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='700px'))\n",
    "eb = widgets.Button(description='Export file', layout=widgets.Layout(width='150px'))\n",
    "eb.on_click(exportFile)\n",
    "\n",
    "interact(selectedFilesChannels,x = widgets.SelectMultiple(options=readings.keys(), description='Select multiple files', selected_labels = selected,layout=widgets.Layout(width='700px')))\n",
    "exportBox = widgets.HBox([exportPath,eb])\n",
    "display(exportBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulas / Calculator\n",
    "Input your formulas into this cell for analysis in the plots below\n",
    "\n",
    "There are formulas for calculating:\n",
    "- *MICS* = Poly(R, H, T) - **MICS_FORMULA**\n",
    "- *Alphasense* = f(Curr, Sens, Zero) - **AD_FORMULA**\n",
    "- *Smoothing* = f(Signal, Window) - **SMOOTH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from formula_utils import *\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def show_device_list(Source):\n",
    "\n",
    "    _devices_select.options = [s for s in list(readings[_test.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def commonChannels(selected):\n",
    "    global commonChannelsList\n",
    "    commonChannelsList = []\n",
    "    print selected\n",
    "    if (len(selected) == 1):\n",
    "        commonChannelsList = readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "    if (len(selected) > 1):\n",
    "        commonChannelsList = readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "        for s in list(selected):\n",
    "            commonChannelsList = list(set(commonChannelsList) & set(readings[_test.value][_devices_select.value]['data'][selected[0]].columns))\n",
    "    _Aterm.options = list(commonChannelsList)\n",
    "    _Aterm.source = selected\n",
    "    _Bterm.options = list(commonChannelsList)\n",
    "    _Bterm.source = selected\n",
    "    _Cterm.options = list(commonChannelsList)\n",
    "    _Cterm.source = selected\n",
    "    _Dterm.options = list(commonChannelsList)\n",
    "    _Dterm.source = selected\n",
    "    \n",
    "def calculateFormula(b):\n",
    "    clear_output()\n",
    "    A = _Aterm.value\n",
    "    B = _Bterm.value\n",
    "    C = _Cterm.value\n",
    "    D = _Dterm.value\n",
    "    Name = _formulaName.value\n",
    "    for s in list(selected):\n",
    "        result = functionFormula(s,A,B,C,D,readings)\n",
    "        readings[_test.value]['devices'][s]['data'][Name] = result\n",
    "    print \"Formula {} Added in test {}\".format(Name, _test.value)\n",
    "    \n",
    "def functionFormula(s, Aname, Bname, Cname, Dname,readings): \n",
    "    calcData = pd.DataFrame()\n",
    "    mergeData = pd.merge(pd.merge(pd.merge(readings[_test.value]['devices'][s]['data'].loc[:,(Aname,)],readings[_test.value]['devices'][s]['data'].loc[:,(Bname,)],left_index=True, right_index=True), readings[_test.value]['devices'][s]['data'].loc[:,(Cname,)], left_index=True, right_index=True),readings[_test.value]['devices'][s]['data'].loc[:,(Dname,)],left_index=True, right_index=True)\n",
    "    calcData[Aname] = mergeData.iloc[:,0] #A\n",
    "    calcData[Bname] = mergeData.iloc[:,1] #B\n",
    "    calcData[Cname] = mergeData.iloc[:,2] #C\n",
    "    calcData[Dname] = mergeData.iloc[:,3] #D\n",
    "    A = calcData[Aname]\n",
    "    B = calcData[Bname]\n",
    "    C = calcData[Cname]\n",
    "    D = calcData[Dname]\n",
    "    result = eval(_formula.value)\n",
    "    return result\n",
    "        \n",
    "selected=tuple()\n",
    "def selectedDevices(Source):\n",
    "    global selected\n",
    "    selected = list(Source)\n",
    "    commonChannels(selected)\n",
    "\n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='400px')\n",
    "_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_device_list, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "_Aterm = widgets.Dropdown(description = 'A', layout=layout)\n",
    "_Bterm = widgets.Dropdown(description = 'B', layout=layout)\n",
    "_Cterm = widgets.Dropdown(description = 'C', layout=layout)\n",
    "_Dterm = widgets.Dropdown(description = 'D', layout=layout)\n",
    "\n",
    "_devices_select = widgets.SelectMultiple(layout=widgets.Layout(width='700px'))\n",
    "#selected_labels = selected\n",
    "_devices_select_drop = interact(selectedDevices,\n",
    "                                 Source = _devices_select)\n",
    "\n",
    "_selectBox = widgets.VBox([_test_drop, _devices_select])\n",
    "\n",
    "_formulaName = widgets.Text(description = 'Name: ')\n",
    "_formula = widgets.Text(description = '=')\n",
    "_ABtermsBox = widgets.HBox([_Aterm, _Bterm])\n",
    "_CDtermsBox = widgets.HBox([_Cterm, _Dterm])\n",
    "_termsBox = widgets.VBox([_selectBox, _ABtermsBox, _CDtermsBox])\n",
    "_calculate = widgets.Button(description='Calculate')\n",
    "_calculateBox = widgets.HBox([_formulaName,_formula, _calculate])\n",
    "_calculate.on_click(calculateFormula)\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Select the Files for your formulas to apply</h4>'))\n",
    "display(_termsBox)\n",
    "display(widgets.HTML('<h4>Input your formula Below</h4>'))\n",
    "display(_calculateBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaSense Baseline Calibration\n",
    "\n",
    "These functions are used to create the alphasense pollutant correction based on Working, Auxiliary and calibration data provided by alphasense. Run the 1.1.1.1 AlphaSense Sensors calibration data cell to load in the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from alphadelta_cal_utils import *\n",
    "\n",
    "selectedTestsAD = tuple()\n",
    "def selectTestAD(x):\n",
    "    global selectedTestsAD\n",
    "    selectedTestsAD = list(x)\n",
    "    \n",
    "def calculateCorrectionAD(b):\n",
    "    clear_output()\n",
    "    for testAD in selectedTestsAD:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testAD]['devices']:\n",
    "            if 'is_reference' in readings[testAD]['devices'][reading]:\n",
    "                print 'Reference found'\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testAD]['devices'][reading]['data']\n",
    "                break\n",
    "            else:\n",
    "                print 'No reference measurement found'\n",
    "                refAvail = False\n",
    "                dataframeRef = ''\n",
    "\n",
    "        for kit in readings[testAD]['devices']:\n",
    "            if 'alphasense' in readings[testAD]['devices'][kit]:\n",
    "                \n",
    "                sensorID = readings[testAD]['devices'][kit]['alphasense']\n",
    "                sensorID_CO = readings[testAD]['devices'][kit]['alphasense']['CO']\n",
    "                sensorID_NO2 = readings[testAD]['devices'][kit]['alphasense']['NO2']\n",
    "                sensorID_OX = readings[testAD]['devices'][kit]['alphasense']['O3']\n",
    "                sensorSlots = readings[testAD]['devices'][kit]['alphasense']['SLOTS']\n",
    "                               \n",
    "                sensorID = (['CO', sensorID_CO, 'classic', '', sensorSlots.index('CO')+1], \n",
    "                            ['NO2', sensorID_NO2, 'baseline', 'single_aux', sensorSlots.index('NO2')+1], \n",
    "                            ['O3', sensorID_OX, 'baseline', 'single_aux', sensorSlots.index('O3')+1])\n",
    "                \n",
    "                # Calculate correction\n",
    "                readings[testAD]['devices'][kit]['data'], CorrParams = calculatePollutants(\n",
    "                        _dataframe = readings[testAD]['devices'][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = 'baseline',\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _deltas = deltas,\n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = True,\n",
    "                        _verbose = False, \n",
    "                        _printStats = True)\n",
    "\n",
    "# Find out which tests have alphasense values\n",
    "testAlphaSense = list()\n",
    "for test in readings:\n",
    "    for kit in readings[test]['devices']:\n",
    "        if 'alphasense' in readings[test]['devices'][kit] and test not in testAlphaSense:\n",
    "            testAlphaSense.append(test)\n",
    "\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing alphasense to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestAD,\n",
    "         x = widgets.SelectMultiple(options=testAlphaSense, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsAD, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionAD)\n",
    "\n",
    "display(calculateCorrection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Correction Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample For stats checks\n",
    "pollutant = 'NO2'\n",
    "display(CorrParams[pollutant])\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig1, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax1.legend(loc='best')\n",
    "    ax1.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax1.set_ylabel('Avg Temp-Hum / day')\n",
    "    ax1.grid(True)\n",
    "    ax2.legend(loc='best')\n",
    "    ax2.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax2.set_ylabel('Avg Temp / day')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    fig2, (ax3, ax4) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax3.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_pollutant'], label = 'Avg Pollutant', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['deltaAuxBas_avg'], label = 'Delta', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['ratioAuxBas_avg'] , label = 'Ratio', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax3.legend(loc='best')\n",
    "    ax3.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax3.set_ylabel('Avg {} / day'.format(pollutant))\n",
    "    ax3.grid(True)\n",
    "    ax4.legend(loc='best')\n",
    "    ax4.set_xlabel('{} Average'.format(pollutant))\n",
    "    ax4.set_ylabel('Offset / Ratio Baseline vs Auxiliary')\n",
    "    ax4.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.figure_factory import create_2d_density\n",
    "import plotly.tools as tls\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Plot Y limits\n",
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "\n",
    "toshow = []\n",
    "axisshow = []\n",
    "# meanTable = []\n",
    "\n",
    "def show_devices(Source):\n",
    "    _device_drop.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    _device_drop.source = Source\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    _max_date.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "\n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "    del axisshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_device_drop.source, _sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        axisshow.append(_axis_drop.value)\n",
    "        \n",
    "    plot_data = readings[toshow[0][0]]['devices'][toshow[0][1]]['data'].loc[:,(toshow[0][2],)]\n",
    "    list_data_primary = []\n",
    "    list_data_secondary = []\n",
    "    list_data_terciary = []\n",
    "    \n",
    "    if b.slice_time:\n",
    "        plot_data = plot_data[plot_data.index > _min_date.value]\n",
    "        plot_data = plot_data[plot_data.index < _max_date.value]\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            plot_data = pd.merge(plot_data, readings[toshow[i][0]]['devices'][toshow[i][1]]['data'].loc[:,(toshow[i][2],)], left_index=True, right_index=True)\n",
    "\n",
    "    print '-------------------------------------'\n",
    "    print ' Medias:\\n'\n",
    "    meanTable = []\n",
    "    for d in toshow:\n",
    "        myMean = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "        meanTable.append(myMean)   \n",
    "    res = plot_data.mean()\n",
    "    for i in range(len(meanTable)): print meanTable[i] + '%.2f' % (res[i])\n",
    "    print '-------------------------------------'\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    for i in range(len(plot_data.columns)):\n",
    "        changed.append(toshow[i][1] + ' - '+ plot_data.columns[i])\n",
    "    plot_data.columns = changed\n",
    "    \n",
    "    subplot_rows = 0\n",
    "    if len(toshow) > 0:\n",
    "        for i in range(len(toshow)):\n",
    "            if axisshow[i]=='1': \n",
    "                list_data_primary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,1)\n",
    "            if axisshow[i]=='2': \n",
    "                list_data_secondary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,2)\n",
    "            if axisshow[i]=='3': \n",
    "                list_data_terciary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,3)\n",
    "          \n",
    "        \n",
    "    fig1 = tls.make_subplots(rows=subplot_rows, cols=1, shared_xaxes=_synchroniseXaxis.value)\n",
    "\n",
    "    #if len(list_data_primary)>0:\n",
    "        #fig1 = plot_data.iplot(kind='scatter', y = list_data_primary, asFigure=True, layout = layout)\n",
    "    #ply.offline.iplot(fig1)\n",
    "    \n",
    "    for i in range(len(list_data_primary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_primary[i]], 'type': 'scatter', 'name': list_data_primary[i]}, 1, 1)\n",
    "\n",
    "    for i in range(len(list_data_secondary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_secondary[i]], 'type': 'scatter', 'name': list_data_secondary[i]}, 2, 1)\n",
    "    \n",
    "    for i in range(len(list_data_terciary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_terciary[i]], 'type': 'scatter', 'name': list_data_terciary[i]}, 3, 1)\n",
    "\n",
    "    if setLimits: \n",
    "        fig1['layout'].update(height = 600,\n",
    "                            legend=dict(x=-.1, y=0.9) ,\n",
    "                           xaxis=dict(title='Time'))\n",
    "                          \n",
    "    else:\n",
    "        fig1['layout'].update(height = 600,\n",
    "                              legend=dict(x=-.1, y=0.9) ,\n",
    "                           xaxis=dict(title='Time'))\n",
    "                           \n",
    "    ply.offline.iplot(fig1)\n",
    "    \n",
    "def reset_time(b):\n",
    "    _min_date.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date.value = readings[b.src.value].index.max()._short_repr\n",
    "\n",
    "layout=widgets.Layout(width='330px')\n",
    "\n",
    "# Test dropdown\n",
    "_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "# Device dropdown\n",
    "_device = widgets.Dropdown(options=[k for k in readings[_test.value]['devices'].keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Device')\n",
    "\n",
    "_device_drop = widgets.interactive(show_sensors, \n",
    "                                Source=_device, \n",
    "                                layout=layout)\n",
    "\n",
    "# Sensor dropdown\n",
    "_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "\n",
    "# Buttons\n",
    "_b_add = widgets.Button(description='Add to Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='120px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "# Axis dropdown\n",
    "_axis_drop = widgets.Dropdown(\n",
    "    options=['1', '2', '3'],\n",
    "    value='1',\n",
    "    description='Subplot:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Synchronise Checkbox\n",
    "_synchroniseXaxis = widgets.Checkbox(value=False, \n",
    "                                     description='Synchronise X axis', \n",
    "                                     disabled=False, \n",
    "                                     layout=widgets.Layout(width='300px'))\n",
    "\n",
    "# Date fields\n",
    "_min_date = widgets.Text(description='Start date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "_max_date = widgets.Text(description='End date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "\n",
    "# Date buttons\n",
    "_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "_b_apply_time.on_click(add_sensor)\n",
    "_b_apply_time.slice_time = True\n",
    "_b_reset_time = _b_reset = widgets.Button(description='Reset dates', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_time.on_click(reset_time)\n",
    "#_b_reset_time.src = _kit\n",
    "\n",
    "\n",
    "_device_box = widgets.HBox([_test_drop, _device_drop])\n",
    "_sensor_box = widgets.HBox([_sensor_drop, _axis_drop, _synchroniseXaxis])\n",
    "_plot_box = widgets.HBox([_b_add , _b_reset_all])\n",
    "_time_box = widgets.HBox([_min_date,_max_date, _b_reset_time, _b_apply_time])\n",
    "_root_box = widgets.VBox([_time_box, _device_box, _sensor_box, _plot_box])\n",
    "display(_root_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Basic Sensor Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Seaborn Correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paint(Source):\n",
    "    clear_output()\n",
    "    sns.set(font_scale=1.4)\n",
    "    g = sns.PairGrid(readings.values()[0])\n",
    "    g = g.map(plt.scatter)\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=layout)\n",
    "_kit_drop = widgets.interactive(paint, Source=_kit, layout=layout)\n",
    "display(_kit_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Seaborn XYPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cropTime = False\n",
    "min_date = \"2001-01-01 00:00:01\"\n",
    "max_date = \"2001-01-01 00:00:01\"\n",
    "doubleAxis = True\n",
    "\n",
    "def show_sensors_A(Source):\n",
    "    A_sensors_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    A_sensors_drop.source = Source\n",
    "    minCropDate.value = readings[Source].index.min()._short_repr\n",
    "    maxCropDate.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def show_sensors_B(Source):\n",
    "    B_sensors_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    B_sensors_drop.source = Source\n",
    "    minCropDate.value = readings[Source].index.min()._short_repr\n",
    "    maxCropDate.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def redraw(b):\n",
    "    cropTime = cropTimeCheck.value\n",
    "    doubleAxis = doubleAxisCheck.value\n",
    "    min_date = minCropDate.value\n",
    "    max_date = maxCropDate.value\n",
    "    mergedData = pd.merge(readings[A_kit.value].loc[:,(A_sensors_drop.value,)], readings[B_kit.value].loc[:,(B_sensors_drop.value,)], left_index=True, right_index=True, suffixes=('_'+A_kit.value, '_'+B_kit.value))\n",
    "    clear_output()\n",
    "    \n",
    "    if cropTime:\n",
    "        mergedData = mergedData[mergedData.index > min_date]\n",
    "        mergedData = mergedData[mergedData.index < max_date]\n",
    "        \n",
    "    #jointplot\n",
    "    df = pd.DataFrame()\n",
    "    A = A_sensors_drop.value + '-' + A_kit.value\n",
    "    B = B_sensors_drop.value + '-' + B_kit.value\n",
    "    df[A] = mergedData.iloc[:,0]\n",
    "    df[B] = mergedData.iloc[:,1]\n",
    "    \n",
    "    sns.set(font_scale=1.3)\n",
    "    sns.jointplot(A, B, data=df, kind=\"reg\", color=\"b\", size=12, scatter_kws={\"s\": 80});\n",
    "    print \"data from \" + str(df.index.min()) + \" to \" + str(df.index.max())                      \n",
    "    pearsonCorr = list(df.corr('pearson')[list(df.columns)[0]])[-1]\n",
    "    print 'Pearson correlation coefficient: ' + str(pearsonCorr)\n",
    "    print 'Coefficient of determination RÂ²: ' + str(pearsonCorr*pearsonCorr)\n",
    "\n",
    "    if cropTime: \n",
    "        \n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                yaxis2=dict(title=B,titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            yaxis2=dict(title=B, titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "        \n",
    "    trace0 = go.Scatter(x=df[A].index, y=df[A], name = A,line = dict(color='rgb(0,97,255)'))\n",
    "    \n",
    "    if (doubleAxis):\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, yaxis='y2', line = dict(color='rgb(255,165,0)'))\n",
    "    else:\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, line = dict(color='rgb(255,165,0)'))\n",
    "    data = [trace0, trace1]\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(figure)\n",
    "\n",
    "  # Delta \n",
    "    delta = df[A]-df[B]\n",
    "    trace0 = go.Scatter(x = df[A].index, y = delta, mode = 'lines')\n",
    "    if cropTime: \n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Delta',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    else:\n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Delta',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    data = [trace0]\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(fig)\n",
    "   \n",
    "    # Ratio\n",
    "    ratio = df[A]*1./df[B]\n",
    "    trace0 = go.Scatter(x = df[A].index, y = ratio, mode = 'lines')\n",
    "    if cropTime: \n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Ratio',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    else:\n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Ratio',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    data = [trace0]\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(fig)\n",
    "    \n",
    "    # Rolling correlation\n",
    "  # fig = plt.figure(figsize=(15,6))\n",
    "  # roll = mergedData.iloc[:,0].rolling(12).corr(mergedData.iloc[:,1])\n",
    "  # trace0 = go.Scatter(x = df[A].index, y = roll, mode = 'lines')\n",
    "  # if cropTime: \n",
    "  #     layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Rolling Average',zerolinecolor='#990000',zerolinewidth=1))\n",
    "  # else:\n",
    "  #     layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Rolling Correlation',zerolinecolor='#990000',zerolinewidth=1))\n",
    "  # data = [trace0]\n",
    "  # fig = go.Figure(data=data, layout=layout)\n",
    "  # ply.offline.iplot(fig)\n",
    "    \n",
    "    # Interactive Correlation\n",
    "\n",
    "  # t = df[A].index\n",
    "  # x = df[A]\n",
    "  # y = df[B]\n",
    "\n",
    "  # trace1 = go.Scatter(\n",
    "  #     x=x, y=y, mode='markers', name='points',\n",
    "  #     marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)\n",
    "  # )\n",
    "  # trace2 = go.Histogram2dcontour(\n",
    "  #     x=x, y=y, name=df[A].column, ncontours=20,\n",
    "  #     colorscale='Hot', reversescale=True, showscale=False\n",
    "  # )\n",
    "  # trace3 = go.Histogram(\n",
    "  #     x=x, name=df[A].column,\n",
    "  #     marker=dict(color='rgb(102,0,0)'),\n",
    "  #     yaxis='y2'\n",
    "  # )\n",
    "  # trace4 = go.Histogram(\n",
    "  #     y=y, name=df[B].column, marker=dict(color='rgb(102,0,0)'),\n",
    "  #     xaxis='x2'\n",
    "  # )\n",
    "  # data = [trace1, trace2, trace3, trace4]\n",
    "\n",
    "  # layout = go.Layout(\n",
    "  #     showlegend=True,\n",
    "  #     autosize=False,\n",
    "  #     width=600,\n",
    "  #     height=550,\n",
    "  #     xaxis=dict(\n",
    "  #         domain=[0, 0.85],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     ),\n",
    "  #     yaxis=dict(\n",
    "  #         domain=[0, 0.85],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     ),\n",
    "  #     margin=dict(\n",
    "  #         t=50\n",
    "  #     ),\n",
    "  #     hovermode='closest',\n",
    "  #     bargap=0,\n",
    "  #     xaxis2=dict(\n",
    "  #         domain=[0.85, 1],\n",
    "  #     showgrid=True,\n",
    "  #     zeroline=False\n",
    "  #     ),\n",
    "  #     yaxis2=dict(\n",
    "  #         domain=[0.85, 1],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     )\n",
    "  # )\n",
    "\n",
    "  # fig = go.Figure(data=data, layout=layout)\n",
    "  # ply.offline.iplot(fig)\n",
    "    \n",
    "if len(readings) < 1: print \"Please load some data first...\"\n",
    "else:\n",
    "    \n",
    "    layout=widgets.Layout(width='350px')\n",
    "    b_redraw = widgets.Button(description='Redraw')\n",
    "    b_redraw.on_click(redraw)\n",
    "    doubleAxisCheck = widgets.Checkbox(value=False, description='Secondary y axis', disabled=False)\n",
    "    \n",
    "    cropTimeCheck = widgets.Checkbox(value=False,description='Crop Data in X axis', disabled=False)\n",
    "    minCropDate = widgets.Text(description='Start date:', layout=layout)\n",
    "    maxCropDate = widgets.Text(description='End date:', layout=layout)\n",
    "    \n",
    "    A_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px') ,value=readings.keys()[0])\n",
    "    A_kit_drop = widgets.interactive(show_sensors_A, Source=A_kit, layout=layout)\n",
    "    A_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    show_sensors_A(readings.keys()[0])\n",
    "    \n",
    "    B_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px'), value=readings.keys()[1])\n",
    "    B_kit_drop = widgets.interactive(show_sensors_B, Source= B_kit, layout=layout)\n",
    "    B_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    show_sensors_B(readings.keys()[1])\n",
    "    \n",
    "    draw_box = widgets.HBox([b_redraw, doubleAxisCheck], layout=widgets.Layout(justify_content='space-between'))\n",
    "    kit_box = widgets.HBox([A_kit, widgets.HTML('<h4><< Data source >></h4>') , B_kit], layout=widgets.Layout(justify_content='space-between'))\n",
    "    sensor_box = widgets.HBox([A_sensors_drop, widgets.HTML('<h4><< Sensor selection >></h4>') , B_sensors_drop], layout=widgets.Layout(justify_content='space-between'))\n",
    "    crop_box = widgets.HBox([cropTimeCheck, minCropDate, maxCropDate], layout=widgets.Layout(justify_content='space-between'))\n",
    "    root_box = widgets.VBox([draw_box, kit_box, sensor_box, crop_box])\n",
    "    \n",
    "    display(root_box)\n",
    "    \n",
    "    #redraw(b_redraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Anomaly Detection\n",
    "\n",
    "Check this here https://annals-csis.org/proceedings/2012/pliks/118.pdf.\n",
    "\n",
    "Below we'll use the Holt-Winters function as defined:\n",
    "\n",
    "$$\\hat y_{max_x}=\\ell_{xâˆ’1}+b_{xâˆ’1}+s_{xâˆ’T}+mâ‹…d_{tâˆ’T}$$\n",
    "\n",
    "$$\\hat y_{min_x}=\\ell_{xâˆ’1}+b_{xâˆ’1}+s_{xâˆ’T}-mâ‹…d_{tâˆ’T}$$\n",
    "\n",
    "$$d_t=\\gammaâˆ£y_tâˆ’\\hat y_tâˆ£+(1âˆ’\\gamma)d_{tâˆ’T},$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "    \n",
    "    \"\"\"\n",
    "    Holt-Winters model with the anomalies detection using Brutlag method\n",
    "    \n",
    "    # series - initial time series\n",
    "    # slen - length of a season\n",
    "    # alpha, beta, gamma - Holt-Winters model coefficients\n",
    "    # n_preds - predictions horizon\n",
    "    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "        \n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "    \n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "          \n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "        \n",
    "        seasonals = self.initial_seasonal_components()\n",
    "        \n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # components initialization\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "                \n",
    "                self.PredictedDeviation.append(0)\n",
    "                \n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                \n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                continue\n",
    "                \n",
    "            if i >= len(self.series): # predicting\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "                \n",
    "                # when predicting we increase uncertainty on each step\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "                \n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "                \n",
    "                # Deviation is calculated according to Brutlag algorithm.\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "                     \n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i%self.slen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Functionality\n",
    "Below, we will define some basic functionality for smoothing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def plotExponentialSmoothing(series, alphas):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Exponential Smoothing\")\n",
    "        plt.grid(True);\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "def plotDoubleExponentialSmoothing(series, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Double Exponential Smoothing\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick** demonstration of smoothing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Smoothing tests\n",
    "dataframe = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "dataframe.columns\n",
    "\n",
    "plotExponentialSmoothing(dataframe['NOX ug/m3'], [0.1])\n",
    "plotDoubleExponentialSmoothing(dataframe['NOX ug/m3'], alphas=[0.05], betas=[0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and data training split\n",
    "\n",
    "The following code uses cross validation on rolling basis structure:\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "from scipy.optimize import minimize              # for function minimization\n",
    "\n",
    "import statsmodels.formula.api as smf            # statistics and econometrics\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "## sklearn Time Series functions and data split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Metrics\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error#, mean_squared_log_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO Select variables here \n",
    "\n",
    "Do Interface for variable selection and reference setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "df = dataframe[['NOX ug/m3', 'NO ug/m3']]\n",
    "df.names = [['NOX', ' NO']]\n",
    "print df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO Preliminary Checks\n",
    "\n",
    "##### TODO Dicker-fuller test (ADF)\n",
    "\n",
    "Use this test to verify **data stationarity**.\n",
    "\n",
    "- Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "- Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n",
    "\n",
    "p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad_fuller_results = sm.tsa.stattools.adfuller(df['NOX ug/m3'])\n",
    "\n",
    "adf = ad_fuller_results[0]\n",
    "pvalue = ad_fuller_results[1]\n",
    "usedlag = ad_fuller_results[2]\n",
    "nobs = ad_fuller_results[3]\n",
    "print 'ADF- Statistic: {}\\npvalue: {}\\nUsed Lag: {}\\nnobs: {}\\n'.format(adf, pvalue, usedlag, nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO Granger Casuality Test\n",
    "\n",
    "Use this test to determine the casuality of variables (which causes the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sm.tsa.stattools.grangercausalitytests(df[['NOX ug/m3','NO ug/m3']].dropna(),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Naive Linear Regression\n",
    "\n",
    "Use this only for basic checking and baseline model setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "\n",
    "df['const']=1\n",
    "print df.head(4)\n",
    "\n",
    "model1=sm.OLS(endog=df['NOX ug/m3'],exog=df[['NO ug/m3','const']])\n",
    "results1=model1.fit()\n",
    "print(results1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Linear Regression with differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['const']=1\n",
    "\n",
    "df['diff_NOX ug/m3']=df['NOX ug/m3'].diff()\n",
    "df['diffNO ug/m3']=df['NO ug/m3'].diff()\n",
    "model2=sm.OLS(endog=df['diff_NOX ug/m3'].dropna(),exog=df[['diffNO ug/m3','const']].dropna())\n",
    "results2=model1.fit()\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO ARIMA(X) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Master = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "Slave = readings['MA04_Formulae_2.csv'].dropna()\n",
    "max_date = min( Master.index[-1], Slave.index[-1])\n",
    " \n",
    "mergedData = pd.merge(Master.loc[:,], Slave.loc[:,], left_index=True, right_index=True)\n",
    "\n",
    "print mergedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predictors = Master[['UMID %' ,'NO ug/m3', 'VV m/s', 'O3 ug/m3']]\n",
    "\n",
    "Predictors = mergedData[['AD_2_TN_smooth (ppb)', 'AlphaDelta Humidity-%']]\n",
    "to_predict = mergedData['NOX ug/m3']\n",
    "\n",
    "TrainingSize = int(0.8*to_predict.shape[0])\n",
    "TestSize = to_predict.shape[0] - TrainingSize\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(to_predict[:TrainingSize],\n",
    "              exog=Predictors[:TrainingSize],\n",
    "              order= (2,1,3),\n",
    "              enforce_invertibility=False,trend='c')\n",
    "                    \n",
    "res = mod.fit(disp=0)\n",
    "                    \n",
    "frc =res.forecast(TestSize,exog=pd.DataFrame(Predictors)[TrainingSize:])\n",
    "\n",
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(mergedData.index[TrainingSize:], frc, 'r')\n",
    "    plt.plot(mergedData.index[TrainingSize:], to_predict[TrainingSize:], 'k.')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"SARIMAX Model Prediction\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Include R in Python Notebook and test it out below - do not modify the first line (%%R -i ...)\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRAN mirror for use in this session\n",
    "# Secure CRAN mirrors\n",
    "\n",
    "#1: 0-Cloud [https]                   2: Algeria [https]\n",
    "#3: Australia (Canberra) [https]      4: Australia (Melbourne 1) [https]\n",
    "#5: Australia (Melbourne 2) [https]   6: Australia (Perth) [https]\n",
    "#7: Austria [https]                   8: Belgium (Ghent) [https]\n",
    "#9: Brazil (PR) [https]              10: Brazil (RJ) [https]\n",
    "#11: Brazil (SP 1) [https]            12: Brazil (SP 2) [https]\n",
    "#13: Bulgaria [https]                 14: Chile 1 [https]\n",
    "#15: Chile 2 [https]                  16: China (Guangzhou) [https]\n",
    "#17: China (Lanzhou) [https]          18: China (Shanghai) [https]\n",
    "#19: Colombia (Cali) [https]          20: Czech Republic [https]\n",
    "#21: Denmark [https]                  22: East Asia [https]\n",
    "#23: Ecuador (Cuenca) [https]         24: Ecuador (Quito) [https]\n",
    "#25: Estonia [https]                  26: France (Lyon 1) [https]\n",
    "#27: France (Lyon 2) [https]          28: France (Marseille) [https]\n",
    "#29: France (Montpellier) [https]     30: France (Paris 2) [https]\n",
    "#31: Germany (Erlangen) [https]       32: Germany (GÃ¶ttingen) [https]\n",
    "#33: Germany (MÃ¼nster) [https]        34: Greece [https]\n",
    "#35: Iceland [https]                  36: India [https]\n",
    "#37: Indonesia (Jakarta) [https]      38: Ireland [https]\n",
    "#39: Italy (Padua) [https]            40: Japan (Tokyo) [https]\n",
    "#41: Japan (Yonezawa) [https]         42: Korea (Ulsan) [https]\n",
    "#43: Malaysia [https]                 44: Mexico (Mexico City) [https]\n",
    "#45: Norway [https]                   46: Philippines [https]\n",
    "#47: Serbia [https]                   48: Spain (A CoruÃ±a) [https]\n",
    "#49: Spain (Madrid) [https]           50: Sweden [https]\n",
    "#51: Switzerland [https]              52: Turkey (Denizli) [https]\n",
    "#53: Turkey (Mersin) [https]          54: UK (Bristol) [https]\n",
    "#55: UK (Cambridge) [https]           56: UK (London 1) [https]\n",
    "#57: USA (CA 1) [https]               58: USA (IA) [https]\n",
    "#59: USA (KS) [https]                 60: USA (MI 1) [https]\n",
    "#61: USA (NY) [https]                 62: USA (OR) [https]\n",
    "#63: USA (TN) [https]                 64: USA (TX 1) [https]\n",
    "#65: Vietnam [https]                  66: (other mirrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "\n",
    "base = rpackages.importr('base')\n",
    "utils = rpackages.importr('utils')\n",
    "# select a mirror for R packages\n",
    "utils.chooseCRANmirror(ind=49) # select the first mirror in the list\n",
    "\n",
    "# R package names\n",
    "packnames = [\"ggplot2\",\n",
    "             \"car\",\n",
    "             \"lattice\",\n",
    "             \"dyn\",\n",
    "             \"dynlm\",\n",
    "             \"zoo\",\n",
    "             \"tseries\",\n",
    "             \"lmtest\",\n",
    "             \"xts\",\n",
    "             \"tidyverse\",\n",
    "             \"lubridate\",\n",
    "             \"lme4\",\n",
    "             \"multcomp\",\n",
    "             \"signal\",\n",
    "             \"ggfortify\"]\n",
    "\n",
    "# Selectively install what needs to be install.\n",
    "for x in packnames:\n",
    "    if not rpackages.isinstalled(x):\n",
    "        utils.install_packages(StrVector(x))\n",
    "\n",
    "# import R's \"GlobalEnv\" to evaluate the function\n",
    "from rpy2.robjects import globalenv\n",
    "\n",
    "# ggplot2 = rpackages.importr('ggplot2')\n",
    "# graphics = rpackages.importr('graphics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in R libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load in the libraries\n",
    "library(\"ggplot2\")\n",
    "library(\"car\")\n",
    "library(\"lattice\")\n",
    "library(\"dyn\")\n",
    "library(\"dynlm\")\n",
    "library(\"zoo\")\n",
    "library(\"tseries\")\n",
    "library(\"lmtest\")\n",
    "library(\"xts\")\n",
    "library(\"tidyverse\")\n",
    "library(\"lubridate\")\n",
    "library(\"lme4\")\n",
    "library(\"multcomp\")\n",
    "library(\"signal\")\n",
    "library('GGally')\n",
    "library('plotly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data to R Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "toshow = []\n",
    "min_date_training = 0\n",
    "max_date_training = 0\n",
    "min_date_eval = 0\n",
    "max_date_eval = 0\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date_training.value = readings[Source].index.min()._short_repr\n",
    "    _max_date_training.value = readings[Source].index.max()._short_repr\n",
    "    _min_date_eval.value = readings[Source].index.min()._short_repr\n",
    "    _max_date_eval.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "    #for k in readings.keys():\n",
    "    #    _min_date_training.value = min(_min_date_training.value, readings[k].index.min()._short_repr)\n",
    "    #    _max_date_training.value = max(_max_date_training.value, readings[k].index.max()._short_repr)\n",
    "    #    _min_date_eval.value = min(_min_date_eval.value,readings[k].index.min()._short_repr)\n",
    "    #    _max_date_eval.value = max(_max_date_eval.value,readings[k].index.max()._short_repr)\n",
    "      \n",
    "    \n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        \n",
    "    global dataframe_export\n",
    "    dataframe_export = readings[toshow[0][0]].loc[:,(toshow[0][1],)]\n",
    "    \n",
    "    #if b.slice_time:\n",
    "    #    dataframe_export = dataframe_export[dataframe_export.index > min(_min_date_training.value,_min_date_eval.value)]\n",
    "    #    dataframe_export = dataframe_export[dataframe_export.index < max(_max_date_training.value,_max_date_eval.value)]\n",
    "    \n",
    "    #for k in readings.keys():\n",
    "    #    _min_date_training.value = min(_min_date_training.value, readings[k].index.min()._short_repr)\n",
    "    #    _max_date_training.value = max(_max_date_training.value, readings[k].index.max()._short_repr)\n",
    "    #    _min_date_eval.value = min(_min_date_eval.value,readings[k].index.min()._short_repr)\n",
    "    #    _max_date_eval.value = max(_max_date_eval.value,readings[k].index.max()._short_repr)\n",
    "    \n",
    "    dataframe_export = dataframe_export[dataframe_export.index > min(_min_date_training.value,_min_date_eval.value)]\n",
    "    dataframe_export = dataframe_export[dataframe_export.index < max(_max_date_training.value,_max_date_eval.value)]\n",
    "    \n",
    "    #print 'Min Date Training / Eval'\n",
    "    #print _min_date_training.value\n",
    "    #print _min_date_eval.value\n",
    "    #print min(_min_date_training.value,_min_date_eval.value)\n",
    "    #print 'Max Date Training / Eval'\n",
    "    #print _max_date_training.value\n",
    "    #print _max_date_eval.value\n",
    "    #print max(_max_date_training.value,_max_date_eval.value)\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            dataframe_export = pd.merge(dataframe_export, readings[toshow[i][0]].loc[:,(toshow[i][1],)], left_index=True, right_index=True)\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    \n",
    "    for i in range(len(dataframe_export.columns)):\n",
    "        changed.append(toshow[i][0] + '-'+ dataframe_export.columns[i])\n",
    "    dataframe_export.columns = changed\n",
    "    \n",
    "    #text=[i  for i in range(len(dataframe_export.columns))]\n",
    "    #for i in range(len(dataframe_export.columns)):\n",
    "    #    item = dataframe_export.columns[i]\n",
    "    #    #print \"data\" + str(i)\n",
    "    #    #print item\n",
    "    #    fileName = item[:item.find('.')]\n",
    "    #    #print fileName\n",
    "    #    channel = item[item.find('-')+1:].split('-')[0]\n",
    "    #    if (len(item[item.find('-')+1:].split('-'))>0):\n",
    "    #        unit = item[item.find('-')+1:].split('-')[1]\n",
    "    #    else:\n",
    "    #        unit = ''\n",
    "    #    #print channel\n",
    "    #    #print unit\n",
    "    #    text[i]='<br>File: '+'{:s}'+str(fileName)+'<br>Channel: '+'{:s}'+str(channel)+\\\n",
    "    #    '<br>Unit: '+'{:s}'+ str(unit)\n",
    "    #    #print text[i]\n",
    "    \n",
    "    fig2 = tls.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    for i in range(len(dataframe_export.columns)):\n",
    "        fig2.append_trace({'x': dataframe_export.index, \n",
    "                          'y': dataframe_export.iloc[:,i], \n",
    "                          'type': 'scatter',\n",
    "                          'name': dataframe_export.columns[i]}, 1, 1)\n",
    "\n",
    "\n",
    "    fig2['layout'].update(\n",
    "        height=800,\n",
    "        showlegend = True,\n",
    "        legend=dict(x=-.1, y=5) ,\n",
    "        xaxis=dict(\n",
    "            rangeslider=dict(),\n",
    "            type='date'\n",
    "        ),\n",
    "        annotations=[dict(\n",
    "                        x=_min_date_training.value,\n",
    "                        y=1,\n",
    "                        xref='x',\n",
    "                        yref='paper',\n",
    "                        text='Training Dataset',\n",
    "                        showarrow=False,\n",
    "                        xanchor=\"left\",\n",
    "                        font=dict(color= 'rgba(44, 160, 101, 1)')\n",
    "                    ),\n",
    "                     dict(\n",
    "                        x=_min_date_eval.value,\n",
    "                        y=0.95,\n",
    "                        xref='x',\n",
    "                        yref='paper',\n",
    "                        text='Evaluation Dataset',\n",
    "                        showarrow=False,\n",
    "                        xanchor=\"left\",\n",
    "                        font=dict(color= 'rgba(160, 160, 0, 1)')\n",
    "                    )\n",
    "                    ],\n",
    "        shapes=[\n",
    "                dict(type='rect',\n",
    "                    layer='below',\n",
    "                    x0=_min_date_training.value,\n",
    "                    x1=_max_date_training.value,\n",
    "                    y0=0.95,\n",
    "                    y1=1,\n",
    "                    yref= \"paper\",\n",
    "                    fillcolor='rgba(44, 160, 0, 0.2)',\n",
    "                    line=dict(color= 'rgba(44, 160, 101,0.6)'),\n",
    "                    ),\n",
    "                dict(type='rect',\n",
    "                    layer='below',\n",
    "                    x0=_min_date_eval.value,\n",
    "                    x1=_max_date_eval.value,\n",
    "                    y0=0.9,\n",
    "                    y1=0.95,\n",
    "                    yref= \"paper\",\n",
    "                    fillcolor='rgba(160, 160, 0, 0.2)',\n",
    "                    line=dict(color= 'rgba(160, 160, 0, 0.6)')\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #fig1 = dataframe_export.iplot(kind='scatter', asFigure=True, layout = layout, hoverinfo='text')\n",
    "    #ply.offline.iplot(fig1)\n",
    "    \n",
    "    print list(dataframe_export.columns.values.tolist())\n",
    "    \n",
    "    ply.offline.iplot(fig2)\n",
    "\n",
    "def reset_time_t(b):\n",
    "    _min_date_training.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date_training.value = readings[b.src.value].index.max()._short_repr\n",
    "    add_sensor(b)\n",
    "    \n",
    "def reset_time_e(b):\n",
    "    _min_date_eval.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date_eval.value = readings[b.src.value].index.max()._short_repr\n",
    "    add_sensor(b)\n",
    "\n",
    "def export_dataFrame(b):\n",
    "    clear_output()\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects import r\n",
    "    \n",
    "    global r_train_dataframe, train_dataframe, r_eval_dataframe, eval_dataframe\n",
    "\n",
    "    train_dataframe = dataframe_export[dataframe_export.index > _min_date_training.value]\n",
    "    train_dataframe = train_dataframe[train_dataframe.index < _max_date_training.value]\n",
    "    train_dataframe.index=train_dataframe.index.to_datetime()\n",
    "    r_train_dataframe = pandas2ri.py2ri(train_dataframe)\n",
    "    \n",
    "    eval_dataframe = dataframe_export[dataframe_export.index > _min_date_eval.value]\n",
    "    eval_dataframe = eval_dataframe[eval_dataframe.index < _max_date_eval.value]\n",
    "    eval_dataframe.index=eval_dataframe.index.to_datetime()\n",
    "    r_eval_dataframe = pandas2ri.py2ri(eval_dataframe)\n",
    "    \n",
    "    %Rpush r_train_dataframe r_eval_dataframe\n",
    "    \n",
    "    print 'Export to R Training dataframe successful with following channels'\n",
    "    %R print(colnames(r_train_dataframe))\n",
    "    min_date_training = _min_date_training.value\n",
    "    max_date_training= _max_date_training.value\n",
    "    \n",
    "    print 'With Date Range'\n",
    "    print min_date_training\n",
    "    print max_date_training\n",
    "    \n",
    "    print ''\n",
    "   \n",
    "    print 'Export to R Evaluation dataframe successful with following channels'\n",
    "    %R print(colnames(r_eval_dataframe))\n",
    "    min_date_eval = str(_min_date_eval.value)\n",
    "    max_date_eval= str(_max_date_eval.value)\n",
    "\n",
    "    print 'With Date Range'\n",
    "    print min_date_eval\n",
    "    print max_date_eval\n",
    "        \n",
    "    refFile = str(_refList.value)\n",
    "    refFile = refFile[:refFile.find('.')]\n",
    "    \n",
    "    print ''\n",
    "    \n",
    "    print 'Reference Dataset'\n",
    "    print refFile\n",
    "    r_train_columns_renamed = False\n",
    "    r_eval_columns_renamed = False\n",
    "    %Rpush refFile r_train_columns_renamed r_eval_columns_renamed\n",
    "\n",
    "_layout=widgets.Layout(width='330px')\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=_layout)\n",
    "_kit_drop = widgets.interactive(show_sensors, Source=_kit, layout=_layout)\n",
    "\n",
    "_b_add = widgets.Button(description='Update plot', layout=widgets.Layout(width='100px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "\n",
    "_sensor_drop = widgets.Dropdown(layout=_layout)\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "_b_reset_time_t = widgets.Button(description='Reset Training Dates', layout=widgets.Layout(width='200px'))\n",
    "_b_reset_time_t.on_click(reset_time_t)\n",
    "_b_reset_time_t.src = _kit\n",
    "\n",
    "_b_reset_time_e = widgets.Button(description='Reset Eval Dates', layout=widgets.Layout(width='200px'))\n",
    "_b_reset_time_e.on_click(reset_time_e)\n",
    "_b_reset_time_e.src = _kit\n",
    "\n",
    "_min_date_training = widgets.Text(description='Start Date Train:', layout=widgets.Layout(width='250px'))\n",
    "_max_date_training = widgets.Text(description='End Date Train:', layout=widgets.Layout(width='250px'))\n",
    "_min_date_eval = widgets.Text(description='Start Date Eval:', layout=widgets.Layout(width='250px'))\n",
    "_max_date_eval = widgets.Text(description='End Date Eval:', layout=widgets.Layout(width='250px'))\n",
    "\n",
    "#_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "#_b_apply_time.on_click(add_sensor)\n",
    "#_b_apply_time.slice_time = True\n",
    "\n",
    "#_b_export = widgets.Button(description='Export to CSV', layout=widgets.Layout(width='150px'))\n",
    "#_b_export.on_click(export_dataFrame)\n",
    "_c_R = widgets.Button(description='Export to R dataframe', layout=widgets.Layout(width='150px'))\n",
    "_c_R.on_click(export_dataFrame)\n",
    "#_exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='600px'))\n",
    "#_fileName = widgets.Text(description = 'Name ', layout=widgets.Layout(width='200px'))\n",
    "\n",
    "_button_box = widgets.HBox([_b_add, _b_reset_all])\n",
    "_sensor_box = widgets.HBox([_kit_drop, _sensor_drop , _button_box])\n",
    "_timeT_box = widgets.HBox([_min_date_training,_max_date_training, _b_reset_time_t])\n",
    "_timeE_box = widgets.HBox([_min_date_eval,_max_date_eval, _b_reset_time_e])\n",
    "\n",
    "#_name_box = widgets.HBox([_b_export, _exportPath, _fileName])\n",
    "#_root_box = widgets.VBox([_time_box, _sensor_box, _name_box, _button_box])\n",
    "\n",
    "_refList = widgets.RadioButtons(\n",
    "    options=[k for k in readings.keys()],\n",
    "    #rows=10,\n",
    "    description='Reference Sensor File',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='400px'),\n",
    ")\n",
    "\n",
    "_cR = widgets.Button(description='Export datasets to R', layout=widgets.Layout(width='200px'))\n",
    "_cR.on_click(export_dataFrame)\n",
    "#_prev_dataset = widgets.Button(description='Preview Datasets', layout=widgets.Layout(width='250px'))\n",
    "#_prev_dataset.on_click(preview_datasets)\n",
    "_button_box = widgets.HBox([_refList,_cR])\n",
    "_root_box = widgets.VBox([_sensor_box, _timeT_box, _timeE_box, _button_box])\n",
    "display(widgets.HTML('<br>'))\n",
    "\n",
    "display(widgets.HTML('<h3>Use this box to create R compatible dataframes</h3>'))\n",
    "display(widgets.HTML('1. Select the signals from each source, and hit Preview Slice'))\n",
    "display(widgets.HTML('2. Apply dates from and to export trim'))\n",
    "display(widgets.HTML('3. Hit export to DataFrame'))\n",
    "display(widgets.HTML('<br>'))\n",
    "display(_root_box)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming and timestamp reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#print(r_train_columns_renamed)\n",
    "#print(refFile)\n",
    "\n",
    "convertNames <- function(dataset){\n",
    "    for (i in colnames(dataset)){\n",
    "        #print(i)\n",
    "        index <- gregexpr('.csv.',i, fixed = TRUE)\n",
    "        #print(index)\n",
    "        #print('-')\n",
    "        fileName=substr(i, start=1, stop=index)\n",
    "        fileName=substr(fileName,start=1,stop=nchar(fileName)-1)\n",
    "        channel=substr(i, start=nchar(fileName)+6, stop=nchar(i))\n",
    "        #print('FileName')\n",
    "        #print(fileName)\n",
    "        #print('Channel')\n",
    "        #print(channel)\n",
    "        \n",
    "        if (fileName==refFile){\n",
    "            #fileName=paste('REF',fileName,sep=\"-\")\n",
    "            fileName='REF'\n",
    "            #print('Reference Dataset')\n",
    "        } else {\n",
    "            #fileName='CORR'\n",
    "        }\n",
    "\n",
    "        if (grepl('Carbon.monoxide.kOhm.ppm.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.kOhm.ppm.sm',channel),nchar(channel)),'KIT_CO_RAW_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide.kOhm.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.kOhm.ppm',channel),nchar(channel)),'KIT_CO_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.sm',channel),nchar(channel)),'KIT_CO_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide',channel),nchar(channel)),'KIT_CO_PPM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.kOhm.ppm.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.kOhm.ppm.sm',channel),nchar(channel)),'KIT_NO2_RAW_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.kOhm.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.kOhm.ppm',channel),nchar(channel)),'KIT_NO2_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.sm',channel),nchar(channel)),'KIT_NO2_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide',channel),nchar(channel)),'KIT_NO2_PPM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity..',channel),nchar(channel)),'AD_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity',channel),nchar(channel)),'AD_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('Humidity..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Humidity..',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Humidity',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Temperature.C',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Temperature.C',channel),nchar(channel)),'AD_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Temperature',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Temperature',channel),nchar(channel)),'AD_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Temperature.C',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Temperature.C',channel),nchar(channel)),'KIT_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Temperature',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Temperature.C',channel),nchar(channel)),'KIT_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Battery..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Battery..',channel),nchar(channel)),'BATT_R',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1W.mV',channel),nchar(channel)),'AD_1W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1A.mV',channel),nchar(channel)),'AD_1A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2W.mV',channel),nchar(channel)),'AD_2W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2A.mV',channel),nchar(channel)),'AD_2A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3W.mV',channel),nchar(channel)),'AD_3W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3A.mV',channel),nchar(channel)),'AD_3A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3A.mV',channel),nchar(channel)),'AD_3A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1cal.ppm',channel),nchar(channel)),'AD_CO_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2cal.ppm',channel),nchar(channel)),'AD_NO2_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3cal.ppm',channel),nchar(channel)),'AD_O3_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta_CO_SM',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta_CO_SM',channel),nchar(channel)),'AD_CO_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.CO',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.CO',channel),nchar(channel)),'AD_CO_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta_NO2_SM',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta_NO2_smooth',channel),nchar(channel)),'AD_NO2_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.NO2',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.NO2',channel),nchar(channel)),'AD_NO2_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.OX',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.OX',channel),nchar(channel)),'AD_O3_PPM_RAW',channel),sep=\"_\")\n",
    "        }\n",
    "\n",
    "        #print(newName)\n",
    "        #name[i]<-newName\n",
    "        #colnames(r_train_dataframe) <- paste(fileName,\"-\",channel)\n",
    "        colnames(dataset)[colnames(dataset) == i] <- newName\n",
    "        #print('----')\n",
    "    }\n",
    "    return(dataset)\n",
    "}\n",
    "\n",
    "if (!r_train_columns_renamed){\n",
    "    print('Time format for Training Dataset')\n",
    "    r_train_dataframe = read.zoo(r_train_dataframe, index = \"Time\",\n",
    "      format = \"%Y-%m-%d %H:%M:00\", tz = \"GMT+2\")\n",
    "    print(time(r_train_dataframe)[1])\n",
    "\n",
    "    r_train_dataframe=convertNames(r_train_dataframe)\n",
    "    r_train_columns_renamed = TRUE \n",
    "}\n",
    "\n",
    "if (!r_eval_columns_renamed){\n",
    "    print('Time format for Evaluation Dataset')\n",
    "    r_eval_dataframe = read.zoo(r_eval_dataframe, index = \"Time\",\n",
    "      format = \"%Y-%m-%d %H:%M:00\", tz = \"GMT+2\")\n",
    "    print(time(r_eval_dataframe)[1])\n",
    "    r_eval_dataframe=convertNames(r_eval_dataframe)\n",
    "    r_eval_columns_renamed = TRUE \n",
    "}\n",
    "\n",
    "print('Renamed Training Dataset Columns')\n",
    "print(colnames(r_train_dataframe))\n",
    "print('Renamed Eval Dataset Columns')\n",
    "print(colnames(r_eval_dataframe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "print(colnames(r_train_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 600\n",
    "options(warn=-1)\n",
    "fileNamePairs = 'KIT_1_4574_MICS'\n",
    "\n",
    "p_train<-ggpairs(data=r_train_dataframe, # data.frame with variables\n",
    "             columns=c(2,3,4,6,8), # columns to plot, default to all.\n",
    "             title=\"Pairs Plot For Training Dataset\")\n",
    "pp_train<-p_train+theme(axis.text=element_text(size=8),\n",
    "        axis.title=element_text(size=6))\n",
    "print(pp_train)\n",
    "\n",
    "#pp_train_plotly<-ggplotly(pp_train)\n",
    "#print(pp_train_plotly)\n",
    "\n",
    "p_eval<-ggpairs(data=r_eval_dataframe, # data.frame with variables\n",
    "             columns=c(2,3,4), # columns to plot, default to all.\n",
    "             title=\"Pairs Plot For Eval Dataset\")\n",
    "pp_eval<-p_eval+theme(axis.text=element_text(size=8),\n",
    "        axis.title=element_text(size=6))\n",
    "print(pp_eval)\n",
    "\n",
    "#pp_eval_plotly <- ggplotly(pp_eval)\n",
    "#print(pp_eval_plotly)\n",
    "\n",
    "options(warn=0)\n",
    "\n",
    "### Eval DataFrame\n",
    "#pairs(~r_eval_dataframe[,\"REF_KIT_CO_RAW\"]+\n",
    "#      r_eval_dataframe[,\"REF_KIT_NO2_RAW\"]+\n",
    "#      r_eval_dataframe[,paste(fileNamePairs,\"KIT_CO_RAW\",sep=\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 700 -h 700\n",
    "\n",
    "p<-coplot(r_train_dataframe[,\"REF_AD_CO_PPM_SM\"] ~ \n",
    "          r_train_dataframe[,\"KIT_1_4574_KIT_CO_RAW_SM\"] | \n",
    "          r_train_dataframe[,\"KIT_1_4574_KIT_H_PRCT\"] + \n",
    "          r_train_dataframe[,\"REF_AD_T_PRCT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(colnames(r_train_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod1_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod1.name = \"NO2_MICS_O(1)\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod1 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod1.reference= r_train_dataframe[,r_mod1_ref]\n",
    "r_train_dataframe.mod1.reference.name = r_mod1_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod1))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod1)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod1), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod1), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod1)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod2_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod2.name = \"NO2_MICS_O(2)\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod2 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM+I(KIT_1_4574_KIT_NO2_RAW_SM^2), \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod2.reference= r_train_dataframe[,r_mod2_ref]\n",
    "r_train_dataframe.mod2.reference.name = r_mod2_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod2))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod2)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod2), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod2), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod2)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod3_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod3.name = \"NO2_MICS_O(1) + H\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod3 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_H_PRCT, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod3.reference= r_train_dataframe[,r_mod3_ref]\n",
    "r_train_dataframe.mod3.reference.name = r_mod3_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod3))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod3)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod3), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod3), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod3)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod4_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod4.name = \"NO2_MICS_O(1) + T\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod4 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_T_C, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod4.reference= r_train_dataframe[,r_mod4_ref]\n",
    "r_train_dataframe.mod4.reference.name = r_mod4_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod4))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod4)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod4), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod4), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod4)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod5_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod5.name = \"NO2_MICS_O(1) + T + H\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod5 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_T_C+KIT_1_4574_KIT_H_PRCT, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod5.reference= r_train_dataframe[,r_mod5_ref]\n",
    "r_train_dataframe.mod5.reference.name = r_mod5_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod5))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod5)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod5), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod5), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod5)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "\n",
    "More into detail for model diagnostics [here](https://www.statmethods.net/stats/rdiagnostics.html)\n",
    "and [here](https://socialsciences.mcmaster.ca/jfox/Courses/Brazil-2009/index.html)\n",
    "\n",
    "All the plots explanations are [here]( https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output).\n",
    "As a rule of a thumb: if Pr(>|t|) is very small, means the value is significant - P-value < 0.05 is OK.\n",
    "\n",
    "Use **Jarque Bera Test** to check if the regression errors are normally \n",
    "distributed for DW test (assumes that the regression errors are normally distributed). The p-value is the probability of the null hypotesis (which for the Jarque Bera Test is that the distribution is normal)\n",
    "\n",
    "Check for autocorrelation of the residuals, with **DW Test** if the residuals are normally distributed: https://stats.stackexchange.com/questions/14914/how-to-test-the-autocorrelation-of-the-residuals\n",
    "\n",
    "Here more information about the plots. [Link](https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864)\n",
    "\n",
    "General rules:\n",
    "\n",
    "- **Residual vs Fitted**: we want a horizontal red line with homogeneus spread. It's a check for the heterodasticity of the distribution. If the residuals change is correlated with the fitted values or the original, our model assumptions are NOK (see https://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions)\n",
    "- **Scale location**: we want the red line horizontal - same check as above (see https://stats.stackexchange.com/questions/52089/what-does-having-constant-variance-in-a-linear-regression-model-mean/52107#52107)\n",
    "- **Normal QQ**: You can interpret a qq-plot analytically by considering the values read from the axes compare for a given plotted point. If the data were well described by a normal distribution, the values should be about the same. For example, take the extreme point at the very far left bottom corner: its x value is somewhere past âˆ’3, but its y value is only a little past âˆ’.2, so it is much further out than it 'should' be. In general, a simple rubric to interpret a qq-plot is that if a given tail twists off counterclockwise from the reference line, there is more data in that tail of your distribution than in a theoretical normal, and if a tail twists off clockwise there is less data in that tail of your distribution than in a theoretical normal. In other words:\n",
    "\n",
    "    - if both tails twist counterclockwise you have heavy tails (leptokurtosis),\n",
    "    - if both tails twist clockwise, you have light tails (platykurtosis),\n",
    "    - if your right tail twists counterclockwise and your left tail twists clockwise, you have - right skew\n",
    "    - if your left tail twists counterclockwise and your right tail twists clockwise, you have left skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Only done for model #3\n",
    "#options(repr.plot.width=6, repr.plot.height=5)\n",
    "## Outlier tests\n",
    "#print('Outlier Tests')\n",
    "#outlierTest(r_train_dataframe.mod1)\n",
    "#leveragePlots(r_train_dataframe.mod1)\n",
    "#qqPlot(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate homoscedasticity\n",
    "## non-constant error variance test\n",
    "#print('Homoscedasticity Tests')\n",
    "#ncvTest(r_train_dataframe.mod1)\n",
    "## plot studentized residuals vs. fitted values \n",
    "#spreadLevelPlot(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate Nonlinearity\n",
    "## component + residual plot \n",
    "#print('Nonlinearity Tests')\n",
    "#crPlots(r_train_dataframe.mod1)\n",
    "## Ceres plots \n",
    "#ceresPlots(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate Collinearity\n",
    "#print('Nonlinearity Tests')\n",
    "#vif(r_train_dataframe.mod1) # variance inflation factors \n",
    "#sqrt(vif(r_train_dataframe.mod1)) > 2 # problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit Plot\n",
    "\n",
    "Use the cells below to plot model data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Traditional R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "plot(r_train_dataframe.mod1.reference ,col=\"black\")\n",
    "grid(5, 5, lwd = 0.85) # grid only in y-direction\n",
    "points(r_train_dataframe.mod1$index, r_train_dataframe.mod1$fit, col=\"red\", type='b')\n",
    "points(r_train_dataframe.mod2$index, r_train_dataframe.mod2$fit, col=\"green\", type='b')\n",
    "points(r_train_dataframe.mod3$index, r_train_dataframe.mod3$fit, col=\"blue\", type='b')\n",
    "points(r_train_dataframe.mod4$index, r_train_dataframe.mod4$fit, col=\"yellow\", type='b')\n",
    "points(r_train_dataframe.mod5$index, r_train_dataframe.mod5$fit, col=\"gray\", type='b')\n",
    "\n",
    "\n",
    "legend('topright', \n",
    "       legend=c(r_train_dataframe.mod1.reference.name, \n",
    "                r_train_dataframe.mod1.name,\n",
    "                r_train_dataframe.mod2.name, \n",
    "                r_train_dataframe.mod3.name,\n",
    "                r_train_dataframe.mod4.name,                \n",
    "                r_train_dataframe.mod5.name), \n",
    "       col=c(\"black\", \"red\", \"green\", \"blue\", \"yellow\", \"gray\"), lty=1:6, cex=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using interactive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Plot data fitting with respect to sample\n",
    "p1 <- plot_ly()\n",
    "\n",
    "## Reference\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe), \n",
    "                y = ~r_train_dataframe.mod1.reference, \n",
    "                name = r_train_dataframe.mod1.reference.name, mode = 'lines')\n",
    "\n",
    "## Model Fit 1\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod1), \n",
    "                y = ~r_train_dataframe.mod1$fit,\n",
    "               name = r_train_dataframe.mod1.name, mode = 'lines')\n",
    "\n",
    "### Model Fit 2\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod2), \n",
    "                y = ~r_train_dataframe.mod2$fit,\n",
    "               name = r_train_dataframe.mod2.name, mode = 'lines')\n",
    "\n",
    "## Model Fit 3\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod3), \n",
    "                y = ~r_train_dataframe.mod3$fit,\n",
    "               name = r_train_dataframe.mod3.name, mode = 'lines')\n",
    "\n",
    "### Model Fit 4\n",
    "#p1 <- add_trace(p1, \n",
    "#                x = time(r_train_dataframe.mod4), \n",
    "#                y = ~r_train_dataframe.mod4$fit,\n",
    "#               name = r_train_dataframe.mod4.name)\n",
    "\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages('h2o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "357px",
    "width": "307px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "485px",
    "left": "63px",
    "top": "107px",
    "width": "335px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
