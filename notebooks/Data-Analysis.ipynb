{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Import/Export\" data-toc-modified-id=\"Data-Import/Export-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Import/Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Calibration-Data\" data-toc-modified-id=\"Load-Calibration-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load Calibration Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#AlphaSense-sensors\" data-toc-modified-id=\"AlphaSense-sensors-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>AlphaSense sensors</a></span></li></ul></li><li><span><a href=\"#Import-Test\" data-toc-modified-id=\"Import-Test-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Import Test</a></span></li><li><span><a href=\"#Import-from-API\" data-toc-modified-id=\"Import-from-API-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Import from API</a></span></li><li><span><a href=\"#Data-Export\" data-toc-modified-id=\"Data-Export-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Data Export</a></span></li></ul></li><li><span><a href=\"#Formulas-/-Calculator\" data-toc-modified-id=\"Formulas-/-Calculator-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Formulas / Calculator</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-shot-tests\" data-toc-modified-id=\"One-shot-tests-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>One-shot tests</a></span></li><li><span><a href=\"#Heating-tests\" data-toc-modified-id=\"Heating-tests-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Heating tests</a></span></li></ul></li><li><span><a href=\"#AlphaSense-Baseline-Calibration\" data-toc-modified-id=\"AlphaSense-Baseline-Calibration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>AlphaSense Baseline Calibration</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO:-Correction-Checks\" data-toc-modified-id=\"TODO:-Correction-Checks-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>TODO: Correction Checks</a></span></li></ul></li><li><span><a href=\"#TODO:-MICS-Baseline-Correction\" data-toc-modified-id=\"TODO:-MICS-Baseline-Correction-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TODO: MICS Baseline Correction</a></span></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Series-Plots\" data-toc-modified-id=\"Time-Series-Plots-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Time Series Plots</a></span></li><li><span><a href=\"#Back2Back-Correlation-Plot\" data-toc-modified-id=\"Back2Back-Correlation-Plot-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Back2Back Correlation Plot</a></span></li><li><span><a href=\"#Full-Seaborn-Correlogram\" data-toc-modified-id=\"Full-Seaborn-Correlogram-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Full Seaborn Correlogram</a></span></li><li><span><a href=\"#TODO:-Anomaly-Detection\" data-toc-modified-id=\"TODO:-Anomaly-Detection-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>TODO: Anomaly Detection</a></span></li></ul></li><li><span><a href=\"#Data-Model\" data-toc-modified-id=\"Data-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Data Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-selection-and-data-training-split\" data-toc-modified-id=\"Feature-selection-and-data-training-split-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Feature selection and data training split</a></span></li><li><span><a href=\"#Feature-selection-and-data-training-split\" data-toc-modified-id=\"Feature-selection-and-data-training-split-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Feature selection and data training split</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO-Preliminary-Checks\" data-toc-modified-id=\"TODO-Preliminary-Checks-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>TODO Preliminary Checks</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO-Dicker-fuller-test-(ADF)\" data-toc-modified-id=\"TODO-Dicker-fuller-test-(ADF)-6.2.1.1\"><span class=\"toc-item-num\">6.2.1.1&nbsp;&nbsp;</span>TODO Dicker-fuller test (ADF)</a></span></li><li><span><a href=\"#TODO-Granger-Casuality-Test\" data-toc-modified-id=\"TODO-Granger-Casuality-Test-6.2.1.2\"><span class=\"toc-item-num\">6.2.1.2&nbsp;&nbsp;</span>TODO Granger Casuality Test</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#R-Framework\" data-toc-modified-id=\"R-Framework-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>R Framework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialise-environment\" data-toc-modified-id=\"Initialise-environment-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Initialise environment</a></span></li><li><span><a href=\"#Install-dependencies\" data-toc-modified-id=\"Install-dependencies-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Install dependencies</a></span></li><li><span><a href=\"#Load-in-R-libraries\" data-toc-modified-id=\"Load-in-R-libraries-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Load in R libraries</a></span></li><li><span><a href=\"#Export-Data-to-R-Dataframe\" data-toc-modified-id=\"Export-Data-to-R-Dataframe-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Export Data to R Dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Renaming-and-timestamp-reading\" data-toc-modified-id=\"Renaming-and-timestamp-reading-7.4.1\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;</span>Renaming and timestamp reading</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pairs-Plot\" data-toc-modified-id=\"Pairs-Plot-7.5.1\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;</span>Pairs Plot</a></span></li><li><span><a href=\"#Coplot\" data-toc-modified-id=\"Coplot-7.5.2\"><span class=\"toc-item-num\">7.5.2&nbsp;&nbsp;</span>Coplot</a></span></li></ul></li><li><span><a href=\"#Model-Iterations\" data-toc-modified-id=\"Model-Iterations-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Model Iterations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Diagnostics\" data-toc-modified-id=\"Model-Diagnostics-7.6.1\"><span class=\"toc-item-num\">7.6.1&nbsp;&nbsp;</span>Model Diagnostics</a></span></li><li><span><a href=\"#Model-Fit-Plot\" data-toc-modified-id=\"Model-Fit-Plot-7.6.2\"><span class=\"toc-item-num\">7.6.2&nbsp;&nbsp;</span>Model Fit Plot</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Traditional-R\" data-toc-modified-id=\"Using-Traditional-R-7.6.2.1\"><span class=\"toc-item-num\">7.6.2.1&nbsp;&nbsp;</span>Using Traditional R</a></span></li><li><span><a href=\"#Using-interactive-Plot\" data-toc-modified-id=\"Using-interactive-Plot-7.6.2.2\"><span class=\"toc-item-num\">7.6.2.2&nbsp;&nbsp;</span>Using interactive Plot</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! pip install jupyter_nbextensions_configurator jupyter_contrib_nbextensions\n",
    "# ! jupyter contrib nbextension install\n",
    "# ! jupyter nbextension install --py fileupload \n",
    "# ! jupyter nbextension enable --py fileupload\n",
    "# ! jupyter nbextension install --py widgetsnbextension \n",
    "# ! jupyter nbextension enable --py widgetsnbextension\n",
    "# ! jupyter nbextensions_configurator enable\n",
    "# ! jupyter nbextension enable codefolding/main\n",
    "# ! jupyter nbextension enable toc2/main\n",
    "! jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run this cell to create a button that hides cells\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    '''\n",
    "    <script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "        if (code_show){\n",
    "            $('div.input').hide();\n",
    "        } else {\n",
    "            $('div.input').show();\n",
    "        }\n",
    "        code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    \n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Jupyter stuff\n",
    "#from IPython.display import display, Markdown, FileLink, FileLinks, clear_output, HTML\n",
    "#from IPython.core.display import HTML\n",
    "#from IPython.display import display, clear_output\n",
    "#import ipywidgets as widgets\n",
    "#from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "## System\n",
    "#import io, pytz, os, time, datetime, fileupload\n",
    "#from shutil import copyfile\n",
    "#from os.path import dirname, join\n",
    "#\n",
    "## Plots\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#\n",
    "#sns.set(color_codes=True)\n",
    "#%matplotlib inline\n",
    "#matplotlib.style.use('seaborn-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Import/Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Calibration Data\n",
    "\n",
    "Sensor calibration data is stored in json files under the [smartcitizen-iscape_data repository](https://github.com/fablabbcn/smartcitizen-iscape-data/tree/internal_dev/calData) and is loaded automatically from the cells below.\n",
    "\n",
    "### AlphaSense sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from calData_utils import getCalData\n",
    "from IPython.display import display\n",
    "\n",
    "alpha_calData = getCalData('alphasense')\n",
    "display(alpha_calData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Test\n",
    "\n",
    "Import test from local test database. \n",
    "\n",
    "**Requirement**:\n",
    "\n",
    "- Include where the directory of your tests is (GIT LFS directory)\n",
    "- Make sure that the desired test is available and has been created with the yaml tool\n",
    "\n",
    "**The cell below will**:\n",
    "\n",
    "- Load all the kits within the test\n",
    "- Check if there were alphasense sensors and retrieve their calibration data and order\n",
    "- Check if there was a reference and convert it units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from test_utils import *\n",
    "\n",
    "#testsDir = os.getcwd()\n",
    "testsDir = '/Users/macoscar/Documents/04_Projects/02_FabLab/02_SmartCitizen/04_iScape/03_Development/03_TestResults/TestStructure'\n",
    "\n",
    "def loadButton(b):\n",
    "    frequency = frequency_text.value + frequency_drop.value\n",
    "    global readings\n",
    "    readings = loadTest()\n",
    "\n",
    "def clearButton(b):\n",
    "    global readings\n",
    "    clearTests()\n",
    "    readings = {}\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Import Local Tests</h4>'))\n",
    "\n",
    "tests = getTests(testsDir)\n",
    "interact(selectTests,\n",
    "         x = widgets.SelectMultiple(options=tests, \n",
    "                           selected_labels = selectedTests, \n",
    "                           layout=widgets.Layout(width='700px')))\n",
    "\n",
    "loadB = widgets.Button(description='Load Local Tests')\n",
    "loadB.on_click(loadButton)\n",
    "\n",
    "frequency_text = widgets.Text(description = 'Frequency',\n",
    "                              value = '10',\n",
    "                              layout = widgets.Layout(width='300px'))\n",
    "frequency_drop = widgets.Dropdown(options = ['H', 'Min', 'S'],\n",
    "                                  value = 'Min',\n",
    "                                  description = '',\n",
    "                                  layout = widgets.Layout(width='100px'))\n",
    "\n",
    "frequency_box = widgets.HBox([frequency_text, frequency_drop])\n",
    "\n",
    "resetB = widgets.Button(description='Clear Tests')\n",
    "resetB.on_click(clearButton)\n",
    "\n",
    "buttonBox = widgets.HBox([loadB, resetB])\n",
    "totalBox = widgets.VBox([frequency_box, buttonBox])\n",
    "display(totalBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_utils import *\n",
    "import re\n",
    "\n",
    "def getDeviceDataInteractive(b):\n",
    "    clear_output()\n",
    "    frequency = frequency_text.value + frequency_drop.value\n",
    "    devices = kitList.value.strip('').split(',')\n",
    "    devicesCorrected = list()\n",
    "    for device in devices: \n",
    "        device = re.sub(' ', '', device)\n",
    "        devicesCorrected.append(device)\n",
    "    test_name = testName.value\n",
    "    # print devicesCorrected\n",
    "    if test_name != '':\n",
    "        try:\n",
    "            readings[test_name] = dict()\n",
    "            readings[test_name] = getReadingsAPI(devicesCorrected, frequency)\n",
    "        except NameError:\n",
    "            global readings\n",
    "            readings = dict()\n",
    "            readings[test_name] = getReadingsAPI(devicesCorrected, frequency)\n",
    "    else:\n",
    "        print 'Input test '\n",
    "\n",
    "def getKitIDInteractive(b):\n",
    "    clear_output()\n",
    "    devices = kitList.value.strip('').split(',')\n",
    "    kitIDs = list()\n",
    "    for device in devices:\n",
    "        kitID = getKitID(device, False)\n",
    "        kitIDs.append(kitID)\n",
    "        print('Device {} has kitID {}'.format(device, kitID))\n",
    "    return kitIDs\n",
    "\n",
    "kitList = widgets.Text(description = 'Kit List: ')\n",
    "testName = widgets.Text(description = 'Input Test Name')\n",
    "\n",
    "getKitIDb = widgets.Button(description='Get Kit ID')\n",
    "getKitIDb.on_click(getKitIDInteractive)\n",
    "\n",
    "loadAPIb = widgets.Button(description='Load API Kit')\n",
    "loadAPIb.on_click(getDeviceDataInteractive)\n",
    "\n",
    "frequency_text = widgets.Text(description = 'Frequency',\n",
    "                              value = '10',\n",
    "                              layout = widgets.Layout(width='300px'))\n",
    "frequency_drop = widgets.Dropdown(options = ['H', 'Min', 'S'],\n",
    "                                  value = 'Min',\n",
    "                                  description = '',\n",
    "                                  layout = widgets.Layout(width='50px'))\n",
    "\n",
    "frequency_box = widgets.HBox([frequency_text, frequency_drop])\n",
    "\n",
    "Hbox = widgets.HBox([kitList, testName])\n",
    "ButtonBox = widgets.HBox([getKitIDb, loadAPIb])\n",
    "Box = widgets.VBox([Hbox, frequency_box, ButtonBox])\n",
    "\n",
    "display(Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "global selected\n",
    "selected = []\n",
    "\n",
    "def selectedFilesChannels(x):\n",
    "    selected = list(x)\n",
    "    \n",
    "selected_export=tuple()\n",
    "def selectedDevices_export(Source):\n",
    "    global selected_export\n",
    "    selected_export = list(Source)\n",
    "    \n",
    "def show_device_export(Source):\n",
    "    _devices_select_export.options = [s for s in list(readings[_test_export.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def exportFile(b):\n",
    "    for i in range(len(selected_export)):\n",
    "        b.f = selected_export[i]\n",
    "        exportDir = exportPath.value\n",
    "        if not os.path.exists(exportDir): os.mkdir(exportDir)\n",
    "        savePath = os.path.join(exportDir, b.f)\n",
    "        if not os.path.exists(savePath):\n",
    "            readings[_test_export.value]['devices'][b.f]['data'].to_csv(savePath + '.csv', sep=\",\")\n",
    "            display(FileLink(savePath))\n",
    "        else:\n",
    "            display(widgets.HTML('File Already exists!'))\n",
    "\n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='400px')\n",
    "_test_export = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_export_drop = widgets.interactive(show_device_export, \n",
    "                                Source=_test_export, \n",
    "                                layout=layout)\n",
    "\n",
    "_devices_select_export = widgets.SelectMultiple(layout=widgets.Layout(width='700px'))\n",
    "_devices_select_export_drop = interact(selectedDevices_export,\n",
    "                                 Source = _devices_select_export)\n",
    "\n",
    "display(widgets.HTML('<h3>Export Files</h3>'))\n",
    "exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='700px'))\n",
    "eb = widgets.Button(description='Export file', layout=widgets.Layout(width='150px'))\n",
    "eb.on_click(exportFile)\n",
    "\n",
    "selectBox = widgets.VBox([_test_export_drop, _devices_select_export])\n",
    "exportBox = widgets.HBox([exportPath,eb])\n",
    "_BOX=widgets.VBox([selectBox, exportBox])\n",
    "display(_BOX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulas / Calculator\n",
    "Input your formulas into this cell for analysis in the plots below\n",
    "\n",
    "There are formulas for calculating:\n",
    "- *MICS* = Poly(R, H, T) - **MICS_FORMULA**\n",
    "- *Alphasense's correction proposal* = f(Curr, Sens, Zero) - **AD_FORMULA**\n",
    "- *Smoothing* = f(Signal, Window) - **SMOOTH**\n",
    "- *Absolute humidity* = f(Temperature, Humidity, Pressure) - **ABS_HUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     29,
     41
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from formula_utils import *\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def show_device_list(Source):\n",
    "    _devices_select.options = [s for s in list(readings[_test.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def commonChannels(selected):\n",
    "    global commonChannelsList\n",
    "    commonChannelsList = []\n",
    "    if (len(selected) == 1):\n",
    "        commonChannelsList = readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "    if (len(selected) > 1):\n",
    "        commonChannelsList = readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "        for s in list(selected):\n",
    "            commonChannelsList = list(set(commonChannelsList) & set(readings[_test.value]['devices'][s]['data'].columns))\n",
    "    _Aterm.options = list(commonChannelsList)\n",
    "    _Aterm.source = selected\n",
    "    _Bterm.options = list(commonChannelsList)\n",
    "    _Bterm.source = selected\n",
    "    _Cterm.options = list(commonChannelsList)\n",
    "    _Cterm.source = selected\n",
    "    _Dterm.options = list(commonChannelsList)\n",
    "    _Dterm.source = selected\n",
    "    \n",
    "def calculateFormula(b):\n",
    "    clear_output()\n",
    "    A = _Aterm.value\n",
    "    B = _Bterm.value\n",
    "    C = _Cterm.value\n",
    "    D = _Dterm.value\n",
    "    Name = _formulaName.value\n",
    "    for s in list(selected):\n",
    "        result = functionFormula(s,A,B,C,D,readings)\n",
    "        readings[_test.value]['devices'][s]['data'][Name] = result\n",
    "    print \"Formula {} Added in test {}\".format(Name, _test.value)\n",
    "    \n",
    "def functionFormula(s, Aname, Bname, Cname, Dname, _readings): \n",
    "    calcData = pd.DataFrame()\n",
    "    mergeData = pd.merge(pd.merge(pd.merge(_readings[_test.value]['devices'][s]['data'].loc[:,(Aname,)],_readings[_test.value]['devices'][s]['data'].loc[:,(Bname,)],left_index=True, right_index=True), _readings[_test.value]['devices'][s]['data'].loc[:,(Cname,)], left_index=True, right_index=True),_readings[_test.value]['devices'][s]['data'].loc[:,(Dname,)],left_index=True, right_index=True)\n",
    "    calcData[Aname] = mergeData.iloc[:,0] #A\n",
    "    calcData[Bname] = mergeData.iloc[:,1] #B\n",
    "    calcData[Cname] = mergeData.iloc[:,2] #C\n",
    "    calcData[Dname] = mergeData.iloc[:,3] #D\n",
    "    A = calcData[Aname]\n",
    "    B = calcData[Bname]\n",
    "    C = calcData[Cname]\n",
    "    D = calcData[Dname]\n",
    "    result = eval(_formula.value)\n",
    "    return result\n",
    "        \n",
    "selected=tuple()\n",
    "def selectedDevices(Source):\n",
    "    global selected\n",
    "    selected = list(Source)\n",
    "    commonChannels(selected)\n",
    "\n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='400px')\n",
    "_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_device_list, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "_Aterm = widgets.Dropdown(description = 'A', layout=layout)\n",
    "_Bterm = widgets.Dropdown(description = 'B', layout=layout)\n",
    "_Cterm = widgets.Dropdown(description = 'C', layout=layout)\n",
    "_Dterm = widgets.Dropdown(description = 'D', layout=layout)\n",
    "\n",
    "_devices_select = widgets.SelectMultiple(layout=widgets.Layout(width='700px'))\n",
    "_devices_select_drop = interact(selectedDevices,\n",
    "                                 Source = _devices_select)\n",
    "\n",
    "_selectBox = widgets.VBox([_test_drop, _devices_select])\n",
    "\n",
    "_formulaName = widgets.Text(description = 'Name: ')\n",
    "_formula = widgets.Text(description = '=')\n",
    "_ABtermsBox = widgets.HBox([_Aterm, _Bterm])\n",
    "_CDtermsBox = widgets.HBox([_Cterm, _Dterm])\n",
    "_termsBox = widgets.VBox([_selectBox, _ABtermsBox, _CDtermsBox])\n",
    "_calculate = widgets.Button(description='Calculate')\n",
    "_calculateBox = widgets.HBox([_formulaName,_formula, _calculate])\n",
    "_calculate.on_click(calculateFormula)\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Select the Files for your formulas to apply</h4>'))\n",
    "display(_termsBox)\n",
    "display(widgets.HTML('<h4>Input your formula Below</h4>'))\n",
    "display(_calculateBox)\n",
    "\n",
    "## Vapour equilibrium: B is temperature in degC, assumed 1013mbar\n",
    "# (1.0007 + 3.46*1e-6*1013)*6.1121*np.exp(17.502*B/(240.97+B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot One-shot tests\n",
    "from signal_utils import plot_oneshots\n",
    "channels_pm = ['PM 1.0', 'PM 2.5', 'PM 10.0']\n",
    "device_one_shot = 'KIT_1_ALTERNATE'\n",
    "plot_oneshots(readings, channels_pm, device_one_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heating tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To run in temperature tests with current that have 'ON_OFF' calculated with greater(current, 20)\n",
    "from signal_utils import split_agnostisise\n",
    "\n",
    "for reading in readings:\n",
    "    print (reading)\n",
    "    dataframeResult = split_agnostisise(readings, reading, 'ON_OFF')\n",
    "    # dataframeResult = split_agnostisise(readings, reading, 'measuring')\n",
    "    readings[reading]['devices']['analysis'] = dict()\n",
    "    readings[reading]['devices']['analysis']['data'] = dict()\n",
    "    readings[reading]['devices']['analysis']['data'] = dataframeResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaSense Baseline Calibration\n",
    "\n",
    "These functions are used to create the alphasense pollutant correction based on Working, Auxiliary and calibration data provided by alphasense. Run the 1.1.1.1 AlphaSense Sensors calibration data cell to load in the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     55
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pollutant_cal_utils import *\n",
    "%matplotlib inline\n",
    "ad_append = 'AD_BASE_CALC'\n",
    "\n",
    "selectedTestsAD = tuple()\n",
    "def selectTestAD(x):\n",
    "    global selectedTestsAD\n",
    "    selectedTestsAD = list(x)\n",
    "    \n",
    "def calculateCorrectionAD(b):\n",
    "    clear_output()\n",
    "    for testAD in selectedTestsAD:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testAD]['devices']:\n",
    "            if 'is_reference' in readings[testAD]['devices'][reading]:\n",
    "                print ('Reference found')\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testAD]['devices'][reading]['data']\n",
    "                break\n",
    "            else:\n",
    "                refAvail = False\n",
    "                dataframeRef = ''\n",
    "\n",
    "        for kit in readings[testAD]['devices']:\n",
    "            if 'alphasense' in readings[testAD]['devices'][kit]:\n",
    "                \n",
    "                sensorID = readings[testAD]['devices'][kit]['alphasense']\n",
    "                sensorID_CO = readings[testAD]['devices'][kit]['alphasense']['CO']\n",
    "                sensorID_NO2 = readings[testAD]['devices'][kit]['alphasense']['NO2']\n",
    "                sensorID_OX = readings[testAD]['devices'][kit]['alphasense']['O3']\n",
    "                sensorSlots = readings[testAD]['devices'][kit]['alphasense']['SLOTS']\n",
    "                               \n",
    "                sensorID = (['CO', sensorID_CO, 'classic', 'single_aux', sensorSlots.index('CO')+1], \n",
    "                            ['NO2', sensorID_NO2, 'baseline', 'single_aux', sensorSlots.index('NO2')+1], \n",
    "                            ['O3', sensorID_OX, 'baseline', 'single_aux', sensorSlots.index('O3')+1])\n",
    "                \n",
    "                # Calculate correction\n",
    "                readings[testAD]['devices'][kit]['data'], CorrParams = calculatePollutantsAlpha(\n",
    "                        _dataframe = readings[testAD]['devices'][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = ad_append,\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _deltas = deltas,\n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = True,\n",
    "                        _verbose = False, \n",
    "                        _printStats = True)\n",
    "\n",
    "# Find out which tests have alphasense values\n",
    "testAlphaSense = list()\n",
    "for test in readings:\n",
    "    for kit in readings[test]['devices']:\n",
    "        if 'alphasense' in readings[test]['devices'][kit] and test not in testAlphaSense:\n",
    "            testAlphaSense.append(test)\n",
    "\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing alphasense to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestAD,\n",
    "         x = widgets.SelectMultiple(options=testAlphaSense, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsAD, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionAD)\n",
    "\n",
    "display(calculateCorrection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Correction Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample For stats checks\n",
    "pollutant = 'NO2'\n",
    "display(CorrParams[pollutant])\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig1, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax1.legend(loc='best')\n",
    "    ax1.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax1.set_ylabel('Avg Temp-Hum / day')\n",
    "    ax1.grid(True)\n",
    "    ax2.legend(loc='best')\n",
    "    ax2.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax2.set_ylabel('Avg Temp / day')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    fig2, (ax3, ax4) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax3.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_pollutant'], label = 'Avg Pollutant', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['deltaAuxBas_avg'], label = 'Delta', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['ratioAuxBas_avg'] , label = 'Ratio', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax3.legend(loc='best')\n",
    "    ax3.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax3.set_ylabel('Avg {} / day'.format(pollutant))\n",
    "    ax3.grid(True)\n",
    "    ax4.legend(loc='best')\n",
    "    ax4.set_xlabel('{} Average'.format(pollutant))\n",
    "    ax4.set_ylabel('Offset / Ratio Baseline vs Auxiliary')\n",
    "    ax4.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: MICS Baseline Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pollutant_cal_utils import *\n",
    "from test_utils import ref_append\n",
    "%matplotlib inline\n",
    "mics_append = 'MICS_BASE_CALC'\n",
    "\n",
    "selectedTestsMICS = tuple()\n",
    "def selectTestMICS(x):\n",
    "    global selectedTestsMICS\n",
    "    selectedTestsMICS = list(x)\n",
    "    \n",
    "def calculateCorrectionMICS(b):\n",
    "    clear_output()\n",
    "    for testMICS in selectedTestsMICS:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testMICS]['devices']:\n",
    "            # If there is reference, use it\n",
    "            if 'is_reference' in readings[testMICS]['devices'][reading]:\n",
    "                print ('Reference found')\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testMICS]['devices'][reading]['data']\n",
    "                break\n",
    "            # If not, at least use alphasense data\n",
    "            elif 'alphasense' in readings[testMICS]['devices'][reading]:\n",
    "                refAvail = True\n",
    "                \n",
    "                dataframeRef = readings[testMICS]['devices'][reading]['data'].loc[:,['CO_' + ad_append, 'NO2_' + ad_append, 'O3_' + ad_append ]]\n",
    "                # Rename to be a reference\n",
    "                for name in dataframeRef.columns:\n",
    "                    namesub = re.sub(ad_append, ref_append, name)\n",
    "                    dataframeRef.rename(columns={name: namesub}, inplace=True)\n",
    "                break\n",
    "            else:\n",
    "                refAvail = False\n",
    "                dataframeRef = ''\n",
    "\n",
    "        for kit in deviceMICS:\n",
    "            if 'mics' in readings[testMICS]['devices'][kit]:\n",
    "                \n",
    "                sensorID = readings[testMICS]['devices'][kit]['mics']               \n",
    "                sensorID = (['CO', sensorID, 'baseline', 'single_temp'], \n",
    "                            ['NO2', sensorID, 'baseline', 'single_temp'])\n",
    "            \n",
    "            # Temporary until better understanding\n",
    "            else:\n",
    "                sensorID = (['CO', 1, 'baseline', 'single_temp'], \n",
    "                            ['NO2', 1, 'baseline', 'single_temp'])\n",
    "                \n",
    "            # Calculate correction\n",
    "            readings[testMICS]['devices'][kit]['data'], CorrParams = calculatePollutantsMICS(\n",
    "                        _dataframe = readings[testMICS]['devices'][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = mics_append,\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _deltas = deltasMICS,\n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = True,\n",
    "                        _verbose = False, \n",
    "                        _printStats = True)\n",
    "\n",
    "# Find out which tests have measured the mics\n",
    "testMICS = list()\n",
    "deviceMICS = list()\n",
    "for test in readings:\n",
    "    for kit in readings[test]['devices']:\n",
    "        columnsTest = readings[test]['devices'][kit]['data'].columns\n",
    "        if ('CO_MICS_RAW' in columnsTest or 'NO2_MICS_RAW' in columnsTest):\n",
    "            if test not in testMICS:\n",
    "                testMICS.append(test)\n",
    "            if kit not in deviceMICS:\n",
    "                deviceMICS.append(kit)\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing MICS to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestMICS,\n",
    "         x = widgets.SelectMultiple(options=testMICS, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsMICS, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionMICS)\n",
    "deltasMICS = np.arange(1,100,1)\n",
    "display(calculateCorrection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     25,
     31,
     37,
     130,
     137,
     141,
     146,
     149,
     154,
     165,
     173
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, FileLink, FileLinks, clear_output, HTML\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.tools as tls\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Plot Y limits\n",
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "\n",
    "toshow = []\n",
    "axisshow = []\n",
    "# meanTable = []\n",
    "\n",
    "def show_devices(Source):\n",
    "    _device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    _device.source = Source\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    _max_date.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "\n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "    del axisshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_device.source, _sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        axisshow.append(_axis_drop.value)\n",
    "        \n",
    "    plot_data = readings[toshow[0][0]]['devices'][toshow[0][1]]['data'].loc[:,(toshow[0][2],)]\n",
    "    list_data_primary = []\n",
    "    list_data_secondary = []\n",
    "    list_data_terciary = []\n",
    "    \n",
    "    if b.slice_time:\n",
    "        plot_data = plot_data[plot_data.index > _min_date.value]\n",
    "        plot_data = plot_data[plot_data.index < _max_date.value]\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            plot_data = pd.merge(plot_data, readings[toshow[i][0]]['devices'][toshow[i][1]]['data'].loc[:,(toshow[i][2],)], left_index=True, right_index=True)\n",
    "\n",
    "    print ('-------------------------------------')\n",
    "    print (' Medias:\\n')\n",
    "    meanTable = []\n",
    "    for d in toshow:\n",
    "        myMean = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "        meanTable.append(myMean)   \n",
    "    res = plot_data.mean()\n",
    "    for i in range(len(meanTable)): print (meanTable[i] + '%.2f' % (res[i]))\n",
    "    print ('-------------------------------------')\n",
    "    \n",
    "    print ('-------------------------------------')\n",
    "    print (' Std Deviation:\\n')\n",
    "    stdTable = []\n",
    "    for d in toshow:\n",
    "        myStd = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "        stdTable.append(myStd)   \n",
    "    std = plot_data.std()\n",
    "    for i in range(len(stdTable)): print stdTable[i] + '%.2f' % (std[i])\n",
    "    print ('-------------------------------------')\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    for i in range(len(plot_data.columns)):\n",
    "        changed.append(toshow[i][0] + ' - '+ toshow[i][1] + ' - '+ plot_data.columns[i])\n",
    "    plot_data.columns = changed\n",
    "    \n",
    "    subplot_rows = 0\n",
    "    if len(toshow) > 0:\n",
    "        for i in range(len(toshow)):\n",
    "            if axisshow[i]=='1': \n",
    "                list_data_primary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,1)\n",
    "            if axisshow[i]=='2': \n",
    "                list_data_secondary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,2)\n",
    "            if axisshow[i]=='3': \n",
    "                list_data_terciary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,3)\n",
    "          \n",
    "        \n",
    "    fig1 = tls.make_subplots(rows=subplot_rows, cols=1, shared_xaxes=_synchroniseXaxis.value)\n",
    "\n",
    "    #if len(list_data_primary)>0:\n",
    "        #fig1 = plot_data.iplot(kind='scatter', y = list_data_primary, asFigure=True, layout = layout)\n",
    "    #ply.offline.iplot(fig1)\n",
    "    \n",
    "    for i in range(len(list_data_primary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_primary[i]], 'type': 'scatter', 'name': list_data_primary[i]}, 1, 1)\n",
    "\n",
    "    for i in range(len(list_data_secondary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_secondary[i]], 'type': 'scatter', 'name': list_data_secondary[i]}, 2, 1)\n",
    "    \n",
    "    for i in range(len(list_data_terciary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_terciary[i]], 'type': 'scatter', 'name': list_data_terciary[i]}, 3, 1)\n",
    "\n",
    "    if setLimits: \n",
    "        fig1['layout'].update(height = 800,\n",
    "                            legend=dict(x=-.1, y=1.2) ,\n",
    "                           xaxis=dict(title='Time'))\n",
    "                          \n",
    "    else:\n",
    "        fig1['layout'].update(height = 800,\n",
    "                              legend=dict(x=-.1, y=1.2) ,\n",
    "                           xaxis=dict(title='Time'))\n",
    "                           \n",
    "    ply.offline.iplot(fig1)\n",
    "    \n",
    "def reset_time(b):\n",
    "    _min_date.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date.value = readings[b.src.value].index.max()._short_repr\n",
    "\n",
    "layout=widgets.Layout(width='330px')\n",
    "\n",
    "# Test dropdown\n",
    "_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "# Device dropdown\n",
    "_device = widgets.Dropdown(layout=layout,\n",
    "                        description = 'Device')\n",
    "\n",
    "_device_drop = widgets.interactive(show_sensors, \n",
    "                                Source=_device, \n",
    "                                layout=layout)\n",
    "\n",
    "# Sensor dropdown\n",
    "_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "\n",
    "# Buttons\n",
    "_b_add = widgets.Button(description='Add to Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='120px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "# Axis dropdown\n",
    "_axis_drop = widgets.Dropdown(\n",
    "    options=['1', '2', '3'],\n",
    "    value='1',\n",
    "    description='Subplot:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Synchronise Checkbox\n",
    "_synchroniseXaxis = widgets.Checkbox(value=False, \n",
    "                                     description='Synchronise X axis', \n",
    "                                     disabled=False, \n",
    "                                     layout=widgets.Layout(width='300px'))\n",
    "\n",
    "# Date fields\n",
    "_min_date = widgets.Text(description='Start date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "_max_date = widgets.Text(description='End date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "\n",
    "# Date buttons\n",
    "_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "_b_apply_time.on_click(add_sensor)\n",
    "_b_apply_time.slice_time = True\n",
    "_b_reset_time = _b_reset = widgets.Button(description='Reset dates', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_time.on_click(reset_time)\n",
    "#_b_reset_time.src = _kit\n",
    "\n",
    "\n",
    "_device_box = widgets.HBox([_test_drop, _device_drop])\n",
    "_sensor_box = widgets.HBox([_sensor_drop, _axis_drop, _synchroniseXaxis])\n",
    "_plot_box = widgets.HBox([_b_add , _b_reset_all])\n",
    "_time_box = widgets.HBox([_min_date,_max_date, _b_reset_time, _b_apply_time])\n",
    "_root_box = widgets.VBox([_time_box, _device_box, _sensor_box, _plot_box])\n",
    "display(_root_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back2Back Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     15,
     21,
     39,
     76,
     109
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cropTime = False\n",
    "min_date = \"2001-01-01 00:00:01\"\n",
    "max_date = \"2001-01-01 00:00:01\"\n",
    "doubleAxis = True\n",
    "\n",
    "\n",
    "def show_devices(Source):\n",
    "    A_device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    A_device.source = Source\n",
    "    B_device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    B_device.source = Source\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "\n",
    "def show_sensors_A(Source):\n",
    "    A_sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    A_sensor_drop.source = Source\n",
    "    minCropDate.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    maxCropDate.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "    \n",
    "def show_sensors_B(Source):\n",
    "    B_sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    B_sensor_drop.source = Source\n",
    "    minCropDate.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    maxCropDate.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr    \n",
    "\n",
    "# def show_sensors_A(Source):\n",
    "#     A_sensors_drop.options = [s for s in list(readings[Source].columns)]\n",
    "#     A_sensors_drop.source = Source\n",
    "#     minCropDate.value = readings[Source].index.min()._short_repr\n",
    "#     maxCropDate.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "# def show_sensors_B(Source):\n",
    "#     B_sensors_drop.options = [s for s in list(readings[Source].columns)]\n",
    "#     B_sensors_drop.source = Source\n",
    "#     minCropDate.value = readings[Source].index.min()._short_repr\n",
    "#     maxCropDate.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def redraw(b):\n",
    "    cropTime = cropTimeCheck.value\n",
    "    doubleAxis = doubleAxisCheck.value\n",
    "    min_date = minCropDate.value\n",
    "    max_date = maxCropDate.value\n",
    "    mergedData = pd.merge(readings[_test.value]['devices'][A_device.value]['data'].loc[:,(A_sensor_drop.value,)], \n",
    "                          readings[_test.value]['devices'][B_device.value]['data'].loc[:,(B_sensor_drop.value,)], \n",
    "                          left_index=True, right_index=True, suffixes=('_'+A_sensor_drop.value, '_'+B_sensor_drop.value))\n",
    "    clear_output()\n",
    "    \n",
    "    if cropTime:\n",
    "        mergedData = mergedData[mergedData.index > min_date]\n",
    "        mergedData = mergedData[mergedData.index < max_date]\n",
    "        \n",
    "    #jointplot\n",
    "    df = pd.DataFrame()\n",
    "    A = A_sensor_drop.value + '-' + A_device.value\n",
    "    B = B_sensor_drop.value + '-' + B_device.value\n",
    "    df[A] = mergedData.iloc[:,0]\n",
    "    df[B] = mergedData.iloc[:,1]\n",
    "    \n",
    "    sns.set(font_scale=1.3)\n",
    "    sns.jointplot(A, B, data=df, kind=\"reg\", color=\"b\", size=12, scatter_kws={\"s\": 80});\n",
    "    print \"data from \" + str(df.index.min()) + \" to \" + str(df.index.max())                      \n",
    "    pearsonCorr = list(df.corr('pearson')[list(df.columns)[0]])[-1]\n",
    "    print 'Pearson correlation coefficient: ' + str(pearsonCorr)\n",
    "    print 'Coefficient of determination R: ' + str(pearsonCorr*pearsonCorr)\n",
    "\n",
    "    if cropTime: \n",
    "        \n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                yaxis2=dict(title=B,titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            yaxis2=dict(title=B, titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "        \n",
    "    trace0 = go.Scatter(x=df[A].index, y=df[A], name = A,line = dict(color='rgb(0,97,255)'))\n",
    "    \n",
    "    if (doubleAxis):\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, yaxis='y2', line = dict(color='rgb(255,165,0)'))\n",
    "    else:\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, line = dict(color='rgb(255,165,0)'))\n",
    "    data = [trace0, trace1]\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(figure)\n",
    "    \n",
    "if len(readings) < 1: print (\"Please load some data first...\")\n",
    "else:\n",
    "    \n",
    "    layout=widgets.Layout(width='350px')\n",
    "    b_redraw = widgets.Button(description='Redraw')\n",
    "    b_redraw.on_click(redraw)\n",
    "    doubleAxisCheck = widgets.Checkbox(value=False, description='Secondary y axis', disabled=False)\n",
    "    \n",
    "    cropTimeCheck = widgets.Checkbox(value=False,description='Crop Data in X axis', disabled=False)\n",
    "    minCropDate = widgets.Text(description='Start date:', layout=layout)\n",
    "    maxCropDate = widgets.Text(description='End date:', layout=layout)\n",
    "    \n",
    "    # Test dropdown\n",
    "    _test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout = widgets.Layout(width='500px'),\n",
    "                        description = 'Test')\n",
    "\n",
    "    _test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test, \n",
    "                                layout = widgets.Layout(width='500px'))\n",
    "\n",
    "    # Device dropdown\n",
    "    A_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device')\n",
    "    \n",
    "    A_device_drop = widgets.interactive(show_sensors_A, \n",
    "                                    Source=A_device, \n",
    "                                    layout=layout)\n",
    "    \n",
    "    B_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device')\n",
    "    \n",
    "    B_device_drop = widgets.interactive(show_sensors_B, \n",
    "                                    Source=B_device, \n",
    "                                    layout=layout)\n",
    "    \n",
    "    # Sensor dropdown\n",
    "    A_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "    \n",
    "    # Sensor dropdown\n",
    "    B_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "    \n",
    "    # A_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px') ,value=readings.keys()[0])\n",
    "    # A_kit_drop = widgets.interactive(show_sensors_A, Source=A_kit, layout=layout)\n",
    "    # A_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    # show_sensors_A(readings.keys()[0])\n",
    "    # \n",
    "    # B_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px'), value=readings.keys()[1])\n",
    "    # B_kit_drop = widgets.interactive(show_sensors_B, Source= B_kit, layout=layout)\n",
    "    # B_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    # show_sensors_B(readings.keys()[1])\n",
    "    \n",
    "    draw_box = widgets.HBox([b_redraw, doubleAxisCheck], layout=widgets.Layout(justify_content='space-between'))\n",
    "    test_box = widgets.HBox([_test_drop], layout = widgets.Layout(width='500px'))\n",
    "    device_box = widgets.HBox([A_device, widgets.HTML('<h4><< Data source >></h4>') , B_device], layout=widgets.Layout(justify_content='space-between'))\n",
    "    sensor_box = widgets.HBox([A_sensor_drop, widgets.HTML('<h4><< Sensor selection >></h4>') , B_sensor_drop], layout=widgets.Layout(justify_content='space-between'))\n",
    "    crop_box = widgets.HBox([cropTimeCheck, minCropDate, maxCropDate], layout=widgets.Layout(justify_content='space-between'))\n",
    "    root_box = widgets.VBox([draw_box, test_box, device_box, sensor_box, crop_box])\n",
    "    \n",
    "    display(root_box)\n",
    "    \n",
    "    #redraw(b_redraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Seaborn Correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paint(Source):\n",
    "    clear_output()\n",
    "    sns.set(font_scale=1.4)\n",
    "    g = sns.PairGrid(readings.values()[0])\n",
    "    g = g.map(plt.scatter)\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=layout)\n",
    "_kit_drop = widgets.interactive(paint, Source=_kit, layout=layout)\n",
    "display(_kit_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Anomaly Detection\n",
    "\n",
    "Check this here https://annals-csis.org/proceedings/2012/pliks/118.pdf.\n",
    "\n",
    "Below we'll use the Holt-Winters function as defined:\n",
    "\n",
    "$$\\hat y_{max_x}=\\ell_{x1}+b_{x1}+s_{xT}+md_{tT}$$\n",
    "\n",
    "$$\\hat y_{min_x}=\\ell_{x1}+b_{x1}+s_{xT}-md_{tT}$$\n",
    "\n",
    "$$d_t=\\gammay_t\\hat y_t+(1\\gamma)d_{tT},$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "    \n",
    "    \"\"\"\n",
    "    Holt-Winters model with the anomalies detection using Brutlag method\n",
    "    \n",
    "    # series - initial time series\n",
    "    # slen - length of a season\n",
    "    # alpha, beta, gamma - Holt-Winters model coefficients\n",
    "    # n_preds - predictions horizon\n",
    "    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "        \n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "    \n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "          \n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "        \n",
    "        seasonals = self.initial_seasonal_components()\n",
    "        \n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # components initialization\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "                \n",
    "                self.PredictedDeviation.append(0)\n",
    "                \n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                \n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                continue\n",
    "                \n",
    "            if i >= len(self.series): # predicting\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "                \n",
    "                # when predicting we increase uncertainty on each step\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "                \n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "                \n",
    "                # Deviation is calculated according to Brutlag algorithm.\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "                     \n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i%self.slen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     9,
     19,
     38,
     58
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def plotExponentialSmoothing(series, alphas):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Exponential Smoothing\")\n",
    "        plt.grid(True);\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "def plotDoubleExponentialSmoothing(series, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Double Exponential Smoothing\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick** demonstration of smoothing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Smoothing tests\n",
    "dataframe = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "dataframe.columns\n",
    "\n",
    "plotExponentialSmoothing(dataframe['NOX ug/m3'], [0.1])\n",
    "plotDoubleExponentialSmoothing(dataframe['NOX ug/m3'], alphas=[0.05], betas=[0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and data training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     111
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combine all data in one dataframe\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder() \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from test_utils import combine_data\n",
    "import ipywidgets as widgets\n",
    "\n",
    "name_combined_data = 'COMBINED_DEVICES'\n",
    "\n",
    "for reading in readings:\n",
    "    if name_combined_data not in readings[reading]['devices']:\n",
    "        dataframe = combine_data(readings[reading]['devices'], True)\n",
    "        readings[reading]['devices'][name_combined_data] = dict()\n",
    "        readings[reading]['devices'][name_combined_data]['data'] = dict()\n",
    "        readings[reading]['devices'][name_combined_data]['data'] = dataframe\n",
    "\n",
    "selectedRef = tuple()\n",
    "def selectRef(x):\n",
    "    global selectedRef\n",
    "    selectedRef = x\n",
    "\n",
    "selectedFeatures = tuple()\n",
    "def selectFeatures(x):\n",
    "    global selectedFeatures\n",
    "    selectedFeatures = list(x)\n",
    "\n",
    "def show_channels_model(x):\n",
    "    list_ref = list()\n",
    "    list_feat = list()\n",
    "\n",
    "    for column in readings[tests.value]['devices'][name_combined_data]['data'].columns:\n",
    "\n",
    "        if any(name in column for name in x):\n",
    "\n",
    "            if 'REF' in column: \n",
    "                list_ref.append(column)\n",
    "            elif ad_append in column:\n",
    "                list_ref.append(column)\n",
    "            list_feat.append(column)\n",
    "    reference_select.options = list_ref\n",
    "    features_select.options = list_feat\n",
    "\n",
    "def show_devices(x):\n",
    "    devices_list = []\n",
    "    for device in readings[tests.value]['devices']:\n",
    "        if device != name_combined_data:\n",
    "            devices_list.append(device)\n",
    "    devices_select.options = devices_list\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def prepare_dataframe_supervised(b):\n",
    "    clear_output()\n",
    "    \n",
    "    # Define some globals\n",
    "    global algorithm_option\n",
    "    global reframed\n",
    "    global scaler\n",
    "    global values\n",
    "    global train_X\n",
    "    global train_y\n",
    "    global test_X\n",
    "    global test_y\n",
    "    global n_predicted_features\n",
    "    global n_features\n",
    "    global n_lags\n",
    "    \n",
    "    # Take the algorithm and number of lags\n",
    "    algorithm_option = algorithm_option_drop.value\n",
    "    n_lags = n_lags_slider.value\n",
    "    \n",
    "    # join List\n",
    "    list_all = list()\n",
    "    list_all.append(selectedRef)\n",
    "    \n",
    "    for feature in selectedFeatures: \n",
    "        if feature != selectedRef:\n",
    "            list_all.append(feature)\n",
    "    \n",
    "    # print list_all\n",
    "    \n",
    "    # get selected values from list\n",
    "    dataframeSupervised = readings[tests.value]['devices'][name_combined_data]['data'].loc[:,list_all]\n",
    "    dataframeSupervised = dataframeSupervised.dropna()\n",
    "    values = dataframeSupervised.values\n",
    "    \n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    if algorithm_option == 'LSTM_lagged_prediction':\n",
    "        n_features = len(list_all)\n",
    "        n_obs = n_lags * n_features\n",
    "        \n",
    "        ## Option example (lag 1 and lagged prediction as feature)\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        # print 'values shape {}'.format(values.shape)\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        # reframe as supervised learning\n",
    "        reframed = series_to_supervised(scaled, n_lags, 1)\n",
    "        # drop columns we don't want\n",
    "        if n_lags == 1:\n",
    "            print 'Dropping columns since lag is 1'\n",
    "            reframed.drop(reframed.columns[range(len(list_all)+1, 2*len(list_all))], axis=1, inplace=True)\n",
    "            n_predicted_features = 1\n",
    "        else:\n",
    "            n_predicted_features = n_features\n",
    "        # print reframed.head(5)\n",
    "\n",
    "        # split into train and test sets\n",
    "        valuesReframed = reframed.values\n",
    "        train = valuesReframed[:n_train_periods, :]\n",
    "        test = valuesReframed[n_train_periods:, :]\n",
    "        # split into input and outputs\n",
    "        train_X, train_y = train[:, :-n_predicted_features], train[:, -n_predicted_features]\n",
    "        test_X, test_y = test[:, :-n_predicted_features], test[:, -n_predicted_features]\n",
    "        print 'Features + Reference length {}, predicted features length: {}'.format(len(list_all), n_predicted_features)\n",
    "        print 'Training X, y and Test X, y shapes before reshaping'\n",
    "        print (train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], n_lags, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], n_lags, n_features))\n",
    "        print 'Training X, y and Test X, y shapes after reshaping'\n",
    "        print (train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        \n",
    "    elif algorithm_option == 'LSTM_regression_single' or algorithm_option == 'LSTM_regression_deep':\n",
    "        n_features = len(list_all) - 1\n",
    "        n_obs = n_lags * n_features\n",
    "        print 'n_features {}'.format(n_features)\n",
    "        print 'n_obs {}'.format(n_obs)\n",
    "        \n",
    "        ## Option sensor 1 (lag 1 and no lagged prediction as feature)\n",
    "        reframed = series_to_supervised(values, n_lags, 1)\n",
    "        \n",
    "        # drop columns we don't want\n",
    "        if n_lags == 1:\n",
    "            reframed = reframed.iloc[:,1:-n_features]\n",
    "            n_predicted_features= 1\n",
    "        else:\n",
    "            # reframed_drop = reframed.iloc[:,1:]\n",
    "            reframed.drop(reframed.columns[range(0,(n_features+1)*n_lags,n_features+1)], axis=1, inplace=True)\n",
    "            reframed.drop(reframed.columns[range(n_obs+1, n_obs+n_features+1)], axis=1, inplace=True)\n",
    "            n_predicted_features = 1\n",
    "            \n",
    "        values_drop = reframed.values\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values_drop)\n",
    "\n",
    "        # split into train and test sets\n",
    "        train = scaled[:n_train_periods, :]\n",
    "        test = scaled[n_train_periods:, :]\n",
    "\n",
    "        # split into input and outputs\n",
    "        train_X, train_y = train[:, :-n_predicted_features], train[:, -n_predicted_features]\n",
    "        test_X, test_y = test[:, :-n_predicted_features], test[:, -n_predicted_features]\n",
    "\n",
    "        print 'Training X, y and Test X, y shapes before reshaping'\n",
    "        print (train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    \n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], n_lags, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], n_lags, n_features))\n",
    "        print 'Training X, y and Test X, y shapes after reshaping'\n",
    "        print (train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    elif algorithm_option == 'linear_regression' or algorithm_option == 'arima':\n",
    "        \n",
    "        if n_lags >0:\n",
    "            for column in dataframeSupervised.columns:\n",
    "                dataframeSupervised[column] = dataframeSupervised[column].diff(n_lags)\n",
    "        dataframeSupervised['const'] = 1\n",
    "        list_all.append('const')\n",
    "        values = dataframeSupervised.values\n",
    "        \n",
    "        train = values[:n_train_periods, :]\n",
    "        test = values[n_train_periods:, :]\n",
    "        \n",
    "        train_X, train_y = train[:, 1:], train[:, 0]\n",
    "        test_X, test_y = test[:, 1:], test[:, 0]\n",
    "        \n",
    "        print 'Training X, y and Test X, y shapes'\n",
    "        print (train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    \n",
    "    print 'DataFrame has been reframed and prepared for supervised learning'\n",
    "    print 'Reference is: {}'.format(selectedRef)\n",
    "    print 'Features are: {}'.format([i for i in selectedFeatures])\n",
    "    print 'Traning X Shape {}, Training Y Shape {}, Test X Shape {}, Test Y Shape {}'.format(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    \n",
    "\n",
    "n_train_periods = 1000\n",
    "_Layout = widgets.Layout(width='700px')\n",
    "algorithm_options = ['linear_regression', 'arima', 'LSTM_lagged_prediction', 'LSTM_regression_single', 'LSTM_regression_deep']\n",
    "algorithm_option_drop = widgets.Dropdown(options = algorithm_options, \n",
    "                                         layout=_Layout,\n",
    "                                         description = 'Algorithm')\n",
    "\n",
    "n_lags_slider = widgets.IntSlider(value=1, min=0, max=10, step=1, description='Lags:',\n",
    "                                  continuous_update=False,\n",
    "                                  orientation='horizontal',\n",
    "                                  readout=True,\n",
    "                                  readout_format='d')\n",
    " \n",
    "# Dropdown test\n",
    "layout = widgets.Layout(width='400px')\n",
    "tests = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=_Layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "tests_drop = widgets.interactive(show_devices, \n",
    "                                x = tests, \n",
    "                                layout=layout)\n",
    "\n",
    "devices_select = widgets.SelectMultiple(description = 'Device',\n",
    "                                        layout=_Layout)\n",
    "\n",
    "devices_select_int = widgets.interactive(show_channels_model,\n",
    "                                         x = devices_select,\n",
    "                                         layout=_Layout)\n",
    "\n",
    "# Multi-select for features and and single select for reference\n",
    "reference_select = widgets.Dropdown(description = 'Select Reference',\n",
    "                                          layout=_Layout)\n",
    "\n",
    "features_select = widgets.SelectMultiple(description = 'Select Features',\n",
    "                                         layout=_Layout)\n",
    "\n",
    "ref_select_int = widgets.interactive(selectRef, x = reference_select)\n",
    "\n",
    "feat_select_int = widgets.interactive(selectFeatures, x = features_select)\n",
    "\n",
    "prep_data = widgets.Button(description='Prepare DataFrame')\n",
    "prep_data.on_click(prepare_dataframe_supervised)\n",
    "\n",
    "_HBox = widgets.HBox([algorithm_option_drop, n_lags_slider])\n",
    "_VBox = widgets.VBox([tests_drop, devices_select_int, reference_select, features_select])\n",
    "_Box = widgets.VBox([_VBox, _HBox, prep_data])\n",
    "display(widgets.HTML('<hr><h4>Data preparation for model</h4>'))\n",
    "display(_Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7,
     22,
     51
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Dropout\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# design network\n",
    "if algorithm_option == 'LSTM_lagged_prediction' or algorithm_option == 'LSTM_regression_single':\n",
    "    model = Sequential()\n",
    "    n_neurons = 100\n",
    "    model.add(LSTM(n_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(LSTM(1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    # plot history\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    \n",
    "elif algorithm_option == 'LSTM_regression_deep':\n",
    "    model = Sequential()\n",
    "    layers = [50, 100, 1]\n",
    "    model.add(LSTM(layers[0], return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(layers[1], return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_dim=layers[2]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "    \n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    # plot history\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    \n",
    "elif algorithm_option == 'linear_regression':\n",
    "    model = sm.OLS(endog = train_y,\n",
    "                   exog = train_X)\n",
    "    \n",
    "    results=model.fit()\n",
    "    print(results.summary())\n",
    "    \n",
    "    inv_yhat = results.predict(exog=test_X)\n",
    "    inv_y = test_y\n",
    "    \n",
    "elif algorithm_option == 'arima':\n",
    "    \n",
    "    model = sm.tsa.SARIMAX(endog = train_y,\n",
    "                           exog = train_X,\n",
    "                           order= (7,0,7),\n",
    "                           enforce_invertibility=False,\n",
    "                           trend='c')\n",
    "                    \n",
    "    results = model.fit(disp=0)\n",
    "                    \n",
    "\n",
    "    inv_y = test_y\n",
    "    prediction = results.get_prediction(full_results=True,\n",
    "                                    alpha=0.05)\n",
    "    \n",
    "    forecast = results.get_forecast(test_y.shape[0], \n",
    "                                    exog=test_X)\n",
    "    \n",
    "    inv_yhat_prediction = prediction.predicted_mean\n",
    "    inv_yhat = forecast.predicted_mean\n",
    "\n",
    "    # Get your prediction intervals by alpha parameter. alpha=0.05 implies 95% CI\n",
    "    inv_yhat_cis_prediction = prediction.conf_int(alpha=0.05)\n",
    "    inv_yhat_cis = forecast.conf_int(alpha=0.05)\n",
    "    print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import concatenate\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# invert scaling for forecast\n",
    "if algorithm_option == 'LSTM_lagged_prediction':\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2] * n_lags))\n",
    "    print 'LSTM_regression model used, scaling back'\n",
    "    if n_predicted_features == 1:\n",
    "        inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "        inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "        inv_yhat = inv_yhat[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "    else: \n",
    "        inv_yhat = concatenate((yhat, test_X[:, -(n_predicted_features-1):]), axis=1)\n",
    "        inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "        inv_yhat = inv_yhat[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_predicted_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]  \n",
    "elif algorithm_option == 'LSTM_regression_deep' or algorithm_option == 'LSTM_regression_single':\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2] * n_lags))\n",
    "    print 'LSTM_regression model used, scaling back'\n",
    "    if n_predicted_features == 1:\n",
    "        inv_yhat = concatenate((test_X[:, :], yhat), axis=1)\n",
    "        inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "        inv_yhat = inv_yhat[:,-1]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_X[:, :], test_y), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,-1]\n",
    "    print inv_y.shape\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# Plot the stuff\n",
    "pyplot.figure(figsize=(15,10))\n",
    "# Actual data\n",
    "pyplot.plot(np.arange(train_X.shape[0],train_X.shape[0] + test_X.shape[0]), inv_y, label = 'Actual')\n",
    "# Forecast\n",
    "pyplot.plot(np.arange(train_X.shape[0],train_X.shape[0] + test_X.shape[0]), inv_yhat, label = 'Forecast')\n",
    "\n",
    "if algorithm_option == 'arima':\n",
    "    # In sample prediction\n",
    "    pyplot.plot(np.arange(0, train_X.shape[0]), inv_yhat_prediction, lw = 1, label = 'In sample Prediction')\n",
    "    # In sample prediction confidence intervals\n",
    "    pyplot.plot(np.arange(0, train_X.shape[0]), inv_yhat_cis_prediction , lw =0.5, color='red', alpha = 0.5, label='SARIMAX prediction CI')\n",
    "    pyplot.fill_between(np.arange(0, train_X.shape[0]), inv_yhat_cis_prediction.iloc[:,0], inv_yhat_cis_prediction.iloc[:,1], alpha = 0.05 )\n",
    "    # Out of sample forecast Confidence intervals\n",
    "    pyplot.plot(np.arange(train_X.shape[0],train_X.shape[0] + test_X.shape[0]),inv_yhat_cis, lw=1, color=\"black\", alpha=0.5, label='SARIMAX forecast CI')\n",
    "    pyplot.fill_between(np.arange(train_X.shape[0],train_X.shape[0] + test_X.shape[0]), inv_yhat_cis.iloc[:, 0], inv_yhat_cis.iloc[:, 1], alpha=0.05)\n",
    "\n",
    "pyplot.legend(loc ='best')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and data training split\n",
    "\n",
    "The following code uses cross validation on rolling basis structure:\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "from scipy.optimize import minimize              # for function minimization\n",
    "\n",
    "import statsmodels.formula.api as smf            # statistics and econometrics\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "## sklearn Time Series functions and data split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Metrics\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error#, mean_squared_log_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Preliminary Checks\n",
    "\n",
    "#### TODO Dicker-fuller test (ADF)\n",
    "\n",
    "Use this test to verify **data stationarity**.\n",
    "\n",
    "- Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "- Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n",
    "\n",
    "p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad_fuller_results = sm.tsa.stattools.adfuller(df['NOX ug/m3'])\n",
    "\n",
    "adf = ad_fuller_results[0]\n",
    "pvalue = ad_fuller_results[1]\n",
    "usedlag = ad_fuller_results[2]\n",
    "nobs = ad_fuller_results[3]\n",
    "print ('ADF- Statistic: {}\\npvalue: {}\\nUsed Lag: {}\\nnobs: {}\\n'.format(adf, pvalue, usedlag, nobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO Granger Casuality Test\n",
    "\n",
    "Use this test to determine the casuality of variables (which causes the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sm.tsa.stattools.grangercausalitytests(df[['NOX ug/m3','NO ug/m3']].dropna(),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Include R in Python Notebook and test it out below - do not modify the first line (%%R -i ...)\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRAN mirror for use in this session\n",
    "# Secure CRAN mirrors\n",
    "\n",
    "#1: 0-Cloud [https]                   2: Algeria [https]\n",
    "#3: Australia (Canberra) [https]      4: Australia (Melbourne 1) [https]\n",
    "#5: Australia (Melbourne 2) [https]   6: Australia (Perth) [https]\n",
    "#7: Austria [https]                   8: Belgium (Ghent) [https]\n",
    "#9: Brazil (PR) [https]              10: Brazil (RJ) [https]\n",
    "#11: Brazil (SP 1) [https]            12: Brazil (SP 2) [https]\n",
    "#13: Bulgaria [https]                 14: Chile 1 [https]\n",
    "#15: Chile 2 [https]                  16: China (Guangzhou) [https]\n",
    "#17: China (Lanzhou) [https]          18: China (Shanghai) [https]\n",
    "#19: Colombia (Cali) [https]          20: Czech Republic [https]\n",
    "#21: Denmark [https]                  22: East Asia [https]\n",
    "#23: Ecuador (Cuenca) [https]         24: Ecuador (Quito) [https]\n",
    "#25: Estonia [https]                  26: France (Lyon 1) [https]\n",
    "#27: France (Lyon 2) [https]          28: France (Marseille) [https]\n",
    "#29: France (Montpellier) [https]     30: France (Paris 2) [https]\n",
    "#31: Germany (Erlangen) [https]       32: Germany (Gttingen) [https]\n",
    "#33: Germany (Mnster) [https]        34: Greece [https]\n",
    "#35: Iceland [https]                  36: India [https]\n",
    "#37: Indonesia (Jakarta) [https]      38: Ireland [https]\n",
    "#39: Italy (Padua) [https]            40: Japan (Tokyo) [https]\n",
    "#41: Japan (Yonezawa) [https]         42: Korea (Ulsan) [https]\n",
    "#43: Malaysia [https]                 44: Mexico (Mexico City) [https]\n",
    "#45: Norway [https]                   46: Philippines [https]\n",
    "#47: Serbia [https]                   48: Spain (A Corua) [https]\n",
    "#49: Spain (Madrid) [https]           50: Sweden [https]\n",
    "#51: Switzerland [https]              52: Turkey (Denizli) [https]\n",
    "#53: Turkey (Mersin) [https]          54: UK (Bristol) [https]\n",
    "#55: UK (Cambridge) [https]           56: UK (London 1) [https]\n",
    "#57: USA (CA 1) [https]               58: USA (IA) [https]\n",
    "#59: USA (KS) [https]                 60: USA (MI 1) [https]\n",
    "#61: USA (NY) [https]                 62: USA (OR) [https]\n",
    "#63: USA (TN) [https]                 64: USA (TX 1) [https]\n",
    "#65: Vietnam [https]                  66: (other mirrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "\n",
    "base = rpackages.importr('base')\n",
    "utils = rpackages.importr('utils')\n",
    "# select a mirror for R packages\n",
    "utils.chooseCRANmirror(ind=49) # select the first mirror in the list\n",
    "\n",
    "# R package names\n",
    "packnames = [\"ggplot2\",\n",
    "             \"car\",\n",
    "             \"lattice\",\n",
    "             \"dyn\",\n",
    "             \"dynlm\",\n",
    "             \"zoo\",\n",
    "             \"tseries\",\n",
    "             \"lmtest\",\n",
    "             \"xts\",\n",
    "             \"tidyverse\",\n",
    "             \"lubridate\",\n",
    "             \"lme4\",\n",
    "             \"multcomp\",\n",
    "             \"signal\",\n",
    "             \"ggfortify\"]\n",
    "\n",
    "# Selectively install what needs to be install.\n",
    "for x in packnames:\n",
    "    if not rpackages.isinstalled(x):\n",
    "        utils.install_packages(StrVector(x))\n",
    "\n",
    "# import R's \"GlobalEnv\" to evaluate the function\n",
    "from rpy2.robjects import globalenv\n",
    "\n",
    "# ggplot2 = rpackages.importr('ggplot2')\n",
    "# graphics = rpackages.importr('graphics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in R libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load in the libraries\n",
    "library(\"ggplot2\")\n",
    "library(\"car\")\n",
    "library(\"lattice\")\n",
    "library(\"dyn\")\n",
    "library(\"dynlm\")\n",
    "library(\"zoo\")\n",
    "library(\"tseries\")\n",
    "library(\"lmtest\")\n",
    "library(\"xts\")\n",
    "library(\"tidyverse\")\n",
    "library(\"lubridate\")\n",
    "library(\"lme4\")\n",
    "library(\"multcomp\")\n",
    "library(\"signal\")\n",
    "library('GGally')\n",
    "library('plotly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data to R Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "toshow = []\n",
    "min_date_training = 0\n",
    "max_date_training = 0\n",
    "min_date_eval = 0\n",
    "max_date_eval = 0\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date_training.value = readings[Source].index.min()._short_repr\n",
    "    _max_date_training.value = readings[Source].index.max()._short_repr\n",
    "    _min_date_eval.value = readings[Source].index.min()._short_repr\n",
    "    _max_date_eval.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "    #for k in readings.keys():\n",
    "    #    _min_date_training.value = min(_min_date_training.value, readings[k].index.min()._short_repr)\n",
    "    #    _max_date_training.value = max(_max_date_training.value, readings[k].index.max()._short_repr)\n",
    "    #    _min_date_eval.value = min(_min_date_eval.value,readings[k].index.min()._short_repr)\n",
    "    #    _max_date_eval.value = max(_max_date_eval.value,readings[k].index.max()._short_repr)\n",
    "      \n",
    "    \n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        \n",
    "    global dataframe_export\n",
    "    dataframe_export = readings[toshow[0][0]].loc[:,(toshow[0][1],)]\n",
    "    \n",
    "    #if b.slice_time:\n",
    "    #    dataframe_export = dataframe_export[dataframe_export.index > min(_min_date_training.value,_min_date_eval.value)]\n",
    "    #    dataframe_export = dataframe_export[dataframe_export.index < max(_max_date_training.value,_max_date_eval.value)]\n",
    "    \n",
    "    #for k in readings.keys():\n",
    "    #    _min_date_training.value = min(_min_date_training.value, readings[k].index.min()._short_repr)\n",
    "    #    _max_date_training.value = max(_max_date_training.value, readings[k].index.max()._short_repr)\n",
    "    #    _min_date_eval.value = min(_min_date_eval.value,readings[k].index.min()._short_repr)\n",
    "    #    _max_date_eval.value = max(_max_date_eval.value,readings[k].index.max()._short_repr)\n",
    "    \n",
    "    dataframe_export = dataframe_export[dataframe_export.index > min(_min_date_training.value,_min_date_eval.value)]\n",
    "    dataframe_export = dataframe_export[dataframe_export.index < max(_max_date_training.value,_max_date_eval.value)]\n",
    "    \n",
    "    #print 'Min Date Training / Eval'\n",
    "    #print _min_date_training.value\n",
    "    #print _min_date_eval.value\n",
    "    #print min(_min_date_training.value,_min_date_eval.value)\n",
    "    #print 'Max Date Training / Eval'\n",
    "    #print _max_date_training.value\n",
    "    #print _max_date_eval.value\n",
    "    #print max(_max_date_training.value,_max_date_eval.value)\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            dataframe_export = pd.merge(dataframe_export, readings[toshow[i][0]].loc[:,(toshow[i][1],)], left_index=True, right_index=True)\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    \n",
    "    for i in range(len(dataframe_export.columns)):\n",
    "        changed.append(toshow[i][0] + '-'+ dataframe_export.columns[i])\n",
    "    dataframe_export.columns = changed\n",
    "    \n",
    "    #text=[i  for i in range(len(dataframe_export.columns))]\n",
    "    #for i in range(len(dataframe_export.columns)):\n",
    "    #    item = dataframe_export.columns[i]\n",
    "    #    #print \"data\" + str(i)\n",
    "    #    #print item\n",
    "    #    fileName = item[:item.find('.')]\n",
    "    #    #print fileName\n",
    "    #    channel = item[item.find('-')+1:].split('-')[0]\n",
    "    #    if (len(item[item.find('-')+1:].split('-'))>0):\n",
    "    #        unit = item[item.find('-')+1:].split('-')[1]\n",
    "    #    else:\n",
    "    #        unit = ''\n",
    "    #    #print channel\n",
    "    #    #print unit\n",
    "    #    text[i]='<br>File: '+'{:s}'+str(fileName)+'<br>Channel: '+'{:s}'+str(channel)+\\\n",
    "    #    '<br>Unit: '+'{:s}'+ str(unit)\n",
    "    #    #print text[i]\n",
    "    \n",
    "    fig2 = tls.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    for i in range(len(dataframe_export.columns)):\n",
    "        fig2.append_trace({'x': dataframe_export.index, \n",
    "                          'y': dataframe_export.iloc[:,i], \n",
    "                          'type': 'scatter',\n",
    "                          'name': dataframe_export.columns[i]}, 1, 1)\n",
    "\n",
    "\n",
    "    fig2['layout'].update(\n",
    "        height=800,\n",
    "        showlegend = True,\n",
    "        legend=dict(x=-.1, y=5) ,\n",
    "        xaxis=dict(\n",
    "            rangeslider=dict(),\n",
    "            type='date'\n",
    "        ),\n",
    "        annotations=[dict(\n",
    "                        x=_min_date_training.value,\n",
    "                        y=1,\n",
    "                        xref='x',\n",
    "                        yref='paper',\n",
    "                        text='Training Dataset',\n",
    "                        showarrow=False,\n",
    "                        xanchor=\"left\",\n",
    "                        font=dict(color= 'rgba(44, 160, 101, 1)')\n",
    "                    ),\n",
    "                     dict(\n",
    "                        x=_min_date_eval.value,\n",
    "                        y=0.95,\n",
    "                        xref='x',\n",
    "                        yref='paper',\n",
    "                        text='Evaluation Dataset',\n",
    "                        showarrow=False,\n",
    "                        xanchor=\"left\",\n",
    "                        font=dict(color= 'rgba(160, 160, 0, 1)')\n",
    "                    )\n",
    "                    ],\n",
    "        shapes=[\n",
    "                dict(type='rect',\n",
    "                    layer='below',\n",
    "                    x0=_min_date_training.value,\n",
    "                    x1=_max_date_training.value,\n",
    "                    y0=0.95,\n",
    "                    y1=1,\n",
    "                    yref= \"paper\",\n",
    "                    fillcolor='rgba(44, 160, 0, 0.2)',\n",
    "                    line=dict(color= 'rgba(44, 160, 101,0.6)'),\n",
    "                    ),\n",
    "                dict(type='rect',\n",
    "                    layer='below',\n",
    "                    x0=_min_date_eval.value,\n",
    "                    x1=_max_date_eval.value,\n",
    "                    y0=0.9,\n",
    "                    y1=0.95,\n",
    "                    yref= \"paper\",\n",
    "                    fillcolor='rgba(160, 160, 0, 0.2)',\n",
    "                    line=dict(color= 'rgba(160, 160, 0, 0.6)')\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #fig1 = dataframe_export.iplot(kind='scatter', asFigure=True, layout = layout, hoverinfo='text')\n",
    "    #ply.offline.iplot(fig1)\n",
    "    \n",
    "    print (list(dataframe_export.columns.values.tolist()))\n",
    "    \n",
    "    ply.offline.iplot(fig2)\n",
    "\n",
    "def reset_time_t(b):\n",
    "    _min_date_training.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date_training.value = readings[b.src.value].index.max()._short_repr\n",
    "    add_sensor(b)\n",
    "    \n",
    "def reset_time_e(b):\n",
    "    _min_date_eval.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date_eval.value = readings[b.src.value].index.max()._short_repr\n",
    "    add_sensor(b)\n",
    "\n",
    "def export_dataFrame(b):\n",
    "    clear_output()\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects import r\n",
    "    \n",
    "    global r_train_dataframe, train_dataframe, r_eval_dataframe, eval_dataframe\n",
    "\n",
    "    train_dataframe = dataframe_export[dataframe_export.index > _min_date_training.value]\n",
    "    train_dataframe = train_dataframe[train_dataframe.index < _max_date_training.value]\n",
    "    train_dataframe.index=train_dataframe.index.to_datetime()\n",
    "    r_train_dataframe = pandas2ri.py2ri(train_dataframe)\n",
    "    \n",
    "    eval_dataframe = dataframe_export[dataframe_export.index > _min_date_eval.value]\n",
    "    eval_dataframe = eval_dataframe[eval_dataframe.index < _max_date_eval.value]\n",
    "    eval_dataframe.index=eval_dataframe.index.to_datetime()\n",
    "    r_eval_dataframe = pandas2ri.py2ri(eval_dataframe)\n",
    "    \n",
    "    %Rpush r_train_dataframe r_eval_dataframe\n",
    "    \n",
    "    print ('Export to R Training dataframe successful with following channels')\n",
    "    %R print(colnames(r_train_dataframe))\n",
    "    min_date_training = _min_date_training.value\n",
    "    max_date_training= _max_date_training.value\n",
    "    \n",
    "    print ('With Date Range')\n",
    "    print (min_date_training)\n",
    "    print (max_date_training)\n",
    "    \n",
    "    print ''\n",
    "   \n",
    "    print ('Export to R Evaluation dataframe successful with following channels')\n",
    "    %R print(colnames(r_eval_dataframe))\n",
    "    min_date_eval = str(_min_date_eval.value)\n",
    "    max_date_eval= str(_max_date_eval.value)\n",
    "\n",
    "    print ('With Date Range')\n",
    "    print (min_date_eval)\n",
    "    print (max_date_eval)\n",
    "        \n",
    "    refFile = str(_refList.value)\n",
    "    refFile = refFile[:refFile.find('.')]\n",
    "    \n",
    "    print ('')\n",
    "    \n",
    "    print ('Reference Dataset')\n",
    "    print (refFile)\n",
    "    r_train_columns_renamed = False\n",
    "    r_eval_columns_renamed = False\n",
    "    %Rpush refFile r_train_columns_renamed r_eval_columns_renamed\n",
    "\n",
    "_layout=widgets.Layout(width='330px')\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=_layout)\n",
    "_kit_drop = widgets.interactive(show_sensors, Source=_kit, layout=_layout)\n",
    "\n",
    "_b_add = widgets.Button(description='Update plot', layout=widgets.Layout(width='100px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "\n",
    "_sensor_drop = widgets.Dropdown(layout=_layout)\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "_b_reset_time_t = widgets.Button(description='Reset Training Dates', layout=widgets.Layout(width='200px'))\n",
    "_b_reset_time_t.on_click(reset_time_t)\n",
    "_b_reset_time_t.src = _kit\n",
    "\n",
    "_b_reset_time_e = widgets.Button(description='Reset Eval Dates', layout=widgets.Layout(width='200px'))\n",
    "_b_reset_time_e.on_click(reset_time_e)\n",
    "_b_reset_time_e.src = _kit\n",
    "\n",
    "_min_date_training = widgets.Text(description='Start Date Train:', layout=widgets.Layout(width='250px'))\n",
    "_max_date_training = widgets.Text(description='End Date Train:', layout=widgets.Layout(width='250px'))\n",
    "_min_date_eval = widgets.Text(description='Start Date Eval:', layout=widgets.Layout(width='250px'))\n",
    "_max_date_eval = widgets.Text(description='End Date Eval:', layout=widgets.Layout(width='250px'))\n",
    "\n",
    "#_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "#_b_apply_time.on_click(add_sensor)\n",
    "#_b_apply_time.slice_time = True\n",
    "\n",
    "#_b_export = widgets.Button(description='Export to CSV', layout=widgets.Layout(width='150px'))\n",
    "#_b_export.on_click(export_dataFrame)\n",
    "_c_R = widgets.Button(description='Export to R dataframe', layout=widgets.Layout(width='150px'))\n",
    "_c_R.on_click(export_dataFrame)\n",
    "#_exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='600px'))\n",
    "#_fileName = widgets.Text(description = 'Name ', layout=widgets.Layout(width='200px'))\n",
    "\n",
    "_button_box = widgets.HBox([_b_add, _b_reset_all])\n",
    "_sensor_box = widgets.HBox([_kit_drop, _sensor_drop , _button_box])\n",
    "_timeT_box = widgets.HBox([_min_date_training,_max_date_training, _b_reset_time_t])\n",
    "_timeE_box = widgets.HBox([_min_date_eval,_max_date_eval, _b_reset_time_e])\n",
    "\n",
    "#_name_box = widgets.HBox([_b_export, _exportPath, _fileName])\n",
    "#_root_box = widgets.VBox([_time_box, _sensor_box, _name_box, _button_box])\n",
    "\n",
    "_refList = widgets.RadioButtons(\n",
    "    options=[k for k in readings.keys()],\n",
    "    #rows=10,\n",
    "    description='Reference Sensor File',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='400px'),\n",
    ")\n",
    "\n",
    "_cR = widgets.Button(description='Export datasets to R', layout=widgets.Layout(width='200px'))\n",
    "_cR.on_click(export_dataFrame)\n",
    "#_prev_dataset = widgets.Button(description='Preview Datasets', layout=widgets.Layout(width='250px'))\n",
    "#_prev_dataset.on_click(preview_datasets)\n",
    "_button_box = widgets.HBox([_refList,_cR])\n",
    "_root_box = widgets.VBox([_sensor_box, _timeT_box, _timeE_box, _button_box])\n",
    "display(widgets.HTML('<br>'))\n",
    "\n",
    "display(widgets.HTML('<h3>Use this box to create R compatible dataframes</h3>'))\n",
    "display(widgets.HTML('1. Select the signals from each source, and hit Preview Slice'))\n",
    "display(widgets.HTML('2. Apply dates from and to export trim'))\n",
    "display(widgets.HTML('3. Hit export to DataFrame'))\n",
    "display(widgets.HTML('<br>'))\n",
    "display(_root_box)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming and timestamp reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#print(r_train_columns_renamed)\n",
    "#print(refFile)\n",
    "\n",
    "convertNames <- function(dataset){\n",
    "    for (i in colnames(dataset)){\n",
    "        #print(i)\n",
    "        index <- gregexpr('.csv.',i, fixed = TRUE)\n",
    "        #print(index)\n",
    "        #print('-')\n",
    "        fileName=substr(i, start=1, stop=index)\n",
    "        fileName=substr(fileName,start=1,stop=nchar(fileName)-1)\n",
    "        channel=substr(i, start=nchar(fileName)+6, stop=nchar(i))\n",
    "        #print('FileName')\n",
    "        #print(fileName)\n",
    "        #print('Channel')\n",
    "        #print(channel)\n",
    "        \n",
    "        if (fileName==refFile){\n",
    "            #fileName=paste('REF',fileName,sep=\"-\")\n",
    "            fileName='REF'\n",
    "            #print('Reference Dataset')\n",
    "        } else {\n",
    "            #fileName='CORR'\n",
    "        }\n",
    "\n",
    "        if (grepl('Carbon.monoxide.kOhm.ppm.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.kOhm.ppm.sm',channel),nchar(channel)),'KIT_CO_RAW_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide.kOhm.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.kOhm.ppm',channel),nchar(channel)),'KIT_CO_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.sm',channel),nchar(channel)),'KIT_CO_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide',channel),nchar(channel)),'KIT_CO_PPM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.kOhm.ppm.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.kOhm.ppm.sm',channel),nchar(channel)),'KIT_NO2_RAW_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.kOhm.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.kOhm.ppm',channel),nchar(channel)),'KIT_NO2_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.sm',channel),nchar(channel)),'KIT_NO2_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide',channel),nchar(channel)),'KIT_NO2_PPM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity..',channel),nchar(channel)),'AD_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity',channel),nchar(channel)),'AD_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('Humidity..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Humidity..',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Humidity',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Temperature.C',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Temperature.C',channel),nchar(channel)),'AD_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Temperature',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Temperature',channel),nchar(channel)),'AD_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Temperature.C',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Temperature.C',channel),nchar(channel)),'KIT_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Temperature',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Temperature.C',channel),nchar(channel)),'KIT_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Battery..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Battery..',channel),nchar(channel)),'BATT_R',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1W.mV',channel),nchar(channel)),'AD_1W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1A.mV',channel),nchar(channel)),'AD_1A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2W.mV',channel),nchar(channel)),'AD_2W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2A.mV',channel),nchar(channel)),'AD_2A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3W.mV',channel),nchar(channel)),'AD_3W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3A.mV',channel),nchar(channel)),'AD_3A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3A.mV',channel),nchar(channel)),'AD_3A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1cal.ppm',channel),nchar(channel)),'AD_CO_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2cal.ppm',channel),nchar(channel)),'AD_NO2_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3cal.ppm',channel),nchar(channel)),'AD_O3_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta_CO_SM',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta_CO_SM',channel),nchar(channel)),'AD_CO_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.CO',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.CO',channel),nchar(channel)),'AD_CO_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta_NO2_SM',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta_NO2_smooth',channel),nchar(channel)),'AD_NO2_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.NO2',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.NO2',channel),nchar(channel)),'AD_NO2_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.OX',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.OX',channel),nchar(channel)),'AD_O3_PPM_RAW',channel),sep=\"_\")\n",
    "        }\n",
    "\n",
    "        #print(newName)\n",
    "        #name[i]<-newName\n",
    "        #colnames(r_train_dataframe) <- paste(fileName,\"-\",channel)\n",
    "        colnames(dataset)[colnames(dataset) == i] <- newName\n",
    "        #print('----')\n",
    "    }\n",
    "    return(dataset)\n",
    "}\n",
    "\n",
    "if (!r_train_columns_renamed){\n",
    "    print('Time format for Training Dataset')\n",
    "    r_train_dataframe = read.zoo(r_train_dataframe, index = \"Time\",\n",
    "      format = \"%Y-%m-%d %H:%M:00\", tz = \"GMT+2\")\n",
    "    print(time(r_train_dataframe)[1])\n",
    "\n",
    "    r_train_dataframe=convertNames(r_train_dataframe)\n",
    "    r_train_columns_renamed = TRUE \n",
    "}\n",
    "\n",
    "if (!r_eval_columns_renamed){\n",
    "    print('Time format for Evaluation Dataset')\n",
    "    r_eval_dataframe = read.zoo(r_eval_dataframe, index = \"Time\",\n",
    "      format = \"%Y-%m-%d %H:%M:00\", tz = \"GMT+2\")\n",
    "    print(time(r_eval_dataframe)[1])\n",
    "    r_eval_dataframe=convertNames(r_eval_dataframe)\n",
    "    r_eval_columns_renamed = TRUE \n",
    "}\n",
    "\n",
    "print('Renamed Training Dataset Columns')\n",
    "print(colnames(r_train_dataframe))\n",
    "print('Renamed Eval Dataset Columns')\n",
    "print(colnames(r_eval_dataframe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "print(colnames(r_train_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 600\n",
    "options(warn=-1)\n",
    "fileNamePairs = 'KIT_1_4574_MICS'\n",
    "\n",
    "p_train<-ggpairs(data=r_train_dataframe, # data.frame with variables\n",
    "             columns=c(2,3,4,6,8), # columns to plot, default to all.\n",
    "             title=\"Pairs Plot For Training Dataset\")\n",
    "pp_train<-p_train+theme(axis.text=element_text(size=8),\n",
    "        axis.title=element_text(size=6))\n",
    "print(pp_train)\n",
    "\n",
    "#pp_train_plotly<-ggplotly(pp_train)\n",
    "#print(pp_train_plotly)\n",
    "\n",
    "p_eval<-ggpairs(data=r_eval_dataframe, # data.frame with variables\n",
    "             columns=c(2,3,4), # columns to plot, default to all.\n",
    "             title=\"Pairs Plot For Eval Dataset\")\n",
    "pp_eval<-p_eval+theme(axis.text=element_text(size=8),\n",
    "        axis.title=element_text(size=6))\n",
    "print(pp_eval)\n",
    "\n",
    "#pp_eval_plotly <- ggplotly(pp_eval)\n",
    "#print(pp_eval_plotly)\n",
    "\n",
    "options(warn=0)\n",
    "\n",
    "### Eval DataFrame\n",
    "#pairs(~r_eval_dataframe[,\"REF_KIT_CO_RAW\"]+\n",
    "#      r_eval_dataframe[,\"REF_KIT_NO2_RAW\"]+\n",
    "#      r_eval_dataframe[,paste(fileNamePairs,\"KIT_CO_RAW\",sep=\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 700 -h 700\n",
    "\n",
    "p<-coplot(r_train_dataframe[,\"REF_AD_CO_PPM_SM\"] ~ \n",
    "          r_train_dataframe[,\"KIT_1_4574_KIT_CO_RAW_SM\"] | \n",
    "          r_train_dataframe[,\"KIT_1_4574_KIT_H_PRCT\"] + \n",
    "          r_train_dataframe[,\"REF_AD_T_PRCT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(colnames(r_train_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod1_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod1.name = \"NO2_MICS_O(1)\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod1 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod1.reference= r_train_dataframe[,r_mod1_ref]\n",
    "r_train_dataframe.mod1.reference.name = r_mod1_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod1))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod1)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod1), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod1), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod1)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod2_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod2.name = \"NO2_MICS_O(2)\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod2 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM+I(KIT_1_4574_KIT_NO2_RAW_SM^2), \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod2.reference= r_train_dataframe[,r_mod2_ref]\n",
    "r_train_dataframe.mod2.reference.name = r_mod2_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod2))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod2)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod2), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod2), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod2)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod3_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod3.name = \"NO2_MICS_O(1) + H\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod3 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_H_PRCT, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod3.reference= r_train_dataframe[,r_mod3_ref]\n",
    "r_train_dataframe.mod3.reference.name = r_mod3_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod3))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod3)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod3), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod3), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod3)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod4_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod4.name = \"NO2_MICS_O(1) + T\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod4 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_T_C, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod4.reference= r_train_dataframe[,r_mod4_ref]\n",
    "r_train_dataframe.mod4.reference.name = r_mod4_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod4))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod4)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod4), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod4), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod4)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod5_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod5.name = \"NO2_MICS_O(1) + T + H\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod5 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_T_C+KIT_1_4574_KIT_H_PRCT, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod5.reference= r_train_dataframe[,r_mod5_ref]\n",
    "r_train_dataframe.mod5.reference.name = r_mod5_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod5))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod5)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod5), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod5), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod5)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "\n",
    "More into detail for model diagnostics [here](https://www.statmethods.net/stats/rdiagnostics.html)\n",
    "and [here](https://socialsciences.mcmaster.ca/jfox/Courses/Brazil-2009/index.html)\n",
    "\n",
    "All the plots explanations are [here]( https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output).\n",
    "As a rule of a thumb: if Pr(>|t|) is very small, means the value is significant - P-value < 0.05 is OK.\n",
    "\n",
    "Use **Jarque Bera Test** to check if the regression errors are normally \n",
    "distributed for DW test (assumes that the regression errors are normally distributed). The p-value is the probability of the null hypotesis (which for the Jarque Bera Test is that the distribution is normal)\n",
    "\n",
    "Check for autocorrelation of the residuals, with **DW Test** if the residuals are normally distributed: https://stats.stackexchange.com/questions/14914/how-to-test-the-autocorrelation-of-the-residuals\n",
    "\n",
    "Here more information about the plots. [Link](https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864)\n",
    "\n",
    "General rules:\n",
    "\n",
    "- **Residual vs Fitted**: we want a horizontal red line with homogeneus spread. It's a check for the heterodasticity of the distribution. If the residuals change is correlated with the fitted values or the original, our model assumptions are NOK (see https://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions)\n",
    "- **Scale location**: we want the red line horizontal - same check as above (see https://stats.stackexchange.com/questions/52089/what-does-having-constant-variance-in-a-linear-regression-model-mean/52107#52107)\n",
    "- **Normal QQ**: You can interpret a qq-plot analytically by considering the values read from the axes compare for a given plotted point. If the data were well described by a normal distribution, the values should be about the same. For example, take the extreme point at the very far left bottom corner: its x value is somewhere past 3, but its y value is only a little past .2, so it is much further out than it 'should' be. In general, a simple rubric to interpret a qq-plot is that if a given tail twists off counterclockwise from the reference line, there is more data in that tail of your distribution than in a theoretical normal, and if a tail twists off clockwise there is less data in that tail of your distribution than in a theoretical normal. In other words:\n",
    "\n",
    "    - if both tails twist counterclockwise you have heavy tails (leptokurtosis),\n",
    "    - if both tails twist clockwise, you have light tails (platykurtosis),\n",
    "    - if your right tail twists counterclockwise and your left tail twists clockwise, you have - right skew\n",
    "    - if your left tail twists counterclockwise and your right tail twists clockwise, you have left skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Only done for model #3\n",
    "#options(repr.plot.width=6, repr.plot.height=5)\n",
    "## Outlier tests\n",
    "#print('Outlier Tests')\n",
    "#outlierTest(r_train_dataframe.mod1)\n",
    "#leveragePlots(r_train_dataframe.mod1)\n",
    "#qqPlot(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate homoscedasticity\n",
    "## non-constant error variance test\n",
    "#print('Homoscedasticity Tests')\n",
    "#ncvTest(r_train_dataframe.mod1)\n",
    "## plot studentized residuals vs. fitted values \n",
    "#spreadLevelPlot(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate Nonlinearity\n",
    "## component + residual plot \n",
    "#print('Nonlinearity Tests')\n",
    "#crPlots(r_train_dataframe.mod1)\n",
    "## Ceres plots \n",
    "#ceresPlots(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate Collinearity\n",
    "#print('Nonlinearity Tests')\n",
    "#vif(r_train_dataframe.mod1) # variance inflation factors \n",
    "#sqrt(vif(r_train_dataframe.mod1)) > 2 # problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit Plot\n",
    "\n",
    "Use the cells below to plot model data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Traditional R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "plot(r_train_dataframe.mod1.reference ,col=\"black\")\n",
    "grid(5, 5, lwd = 0.85) # grid only in y-direction\n",
    "points(r_train_dataframe.mod1$index, r_train_dataframe.mod1$fit, col=\"red\", type='b')\n",
    "points(r_train_dataframe.mod2$index, r_train_dataframe.mod2$fit, col=\"green\", type='b')\n",
    "points(r_train_dataframe.mod3$index, r_train_dataframe.mod3$fit, col=\"blue\", type='b')\n",
    "points(r_train_dataframe.mod4$index, r_train_dataframe.mod4$fit, col=\"yellow\", type='b')\n",
    "points(r_train_dataframe.mod5$index, r_train_dataframe.mod5$fit, col=\"gray\", type='b')\n",
    "\n",
    "\n",
    "legend('topright', \n",
    "       legend=c(r_train_dataframe.mod1.reference.name, \n",
    "                r_train_dataframe.mod1.name,\n",
    "                r_train_dataframe.mod2.name, \n",
    "                r_train_dataframe.mod3.name,\n",
    "                r_train_dataframe.mod4.name,                \n",
    "                r_train_dataframe.mod5.name), \n",
    "       col=c(\"black\", \"red\", \"green\", \"blue\", \"yellow\", \"gray\"), lty=1:6, cex=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using interactive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Plot data fitting with respect to sample\n",
    "p1 <- plot_ly()\n",
    "\n",
    "## Reference\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe), \n",
    "                y = ~r_train_dataframe.mod1.reference, \n",
    "                name = r_train_dataframe.mod1.reference.name, mode = 'lines')\n",
    "\n",
    "## Model Fit 1\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod1), \n",
    "                y = ~r_train_dataframe.mod1$fit,\n",
    "               name = r_train_dataframe.mod1.name, mode = 'lines')\n",
    "\n",
    "### Model Fit 2\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod2), \n",
    "                y = ~r_train_dataframe.mod2$fit,\n",
    "               name = r_train_dataframe.mod2.name, mode = 'lines')\n",
    "\n",
    "## Model Fit 3\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod3), \n",
    "                y = ~r_train_dataframe.mod3$fit,\n",
    "               name = r_train_dataframe.mod3.name, mode = 'lines')\n",
    "\n",
    "### Model Fit 4\n",
    "#p1 <- add_trace(p1, \n",
    "#                x = time(r_train_dataframe.mod4), \n",
    "#                y = ~r_train_dataframe.mod4$fit,\n",
    "#               name = r_train_dataframe.mod4.name)\n",
    "\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages('h2o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Import/Export\" data-toc-modified-id=\"Data-Import/Export-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Import/Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Calibration-Data\" data-toc-modified-id=\"Load-Calibration-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load Calibration Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#AlphaSense-sensors\" data-toc-modified-id=\"AlphaSense-sensors-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>AlphaSense sensors</a></span></li></ul></li><li><span><a href=\"#Import-Local-CSV\" data-toc-modified-id=\"Import-Local-CSV-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Import Local CSV</a></span></li><li><span><a href=\"#Import-Test\" data-toc-modified-id=\"Import-Test-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Import Test</a></span></li><li><span><a href=\"#Import-from-API\" data-toc-modified-id=\"Import-from-API-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Import from API</a></span></li><li><span><a href=\"#Data-Export\" data-toc-modified-id=\"Data-Export-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Data Export</a></span></li></ul></li><li><span><a href=\"#Formulas-/-Calculator\" data-toc-modified-id=\"Formulas-/-Calculator-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Formulas / Calculator</a></span></li><li><span><a href=\"#AlphaSense-Baseline-Calibration\" data-toc-modified-id=\"AlphaSense-Baseline-Calibration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>AlphaSense Baseline Calibration</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO:-Correction-Checks\" data-toc-modified-id=\"TODO:-Correction-Checks-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>TODO: Correction Checks</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Series-Plots\" data-toc-modified-id=\"Time-Series-Plots-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Time Series Plots</a></span></li><li><span><a href=\"#TODO:-Basic-Sensor-Correlations\" data-toc-modified-id=\"TODO:-Basic-Sensor-Correlations-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>TODO: Basic Sensor Correlations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Full-Seaborn-Correlogram\" data-toc-modified-id=\"Full-Seaborn-Correlogram-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Full Seaborn Correlogram</a></span></li><li><span><a href=\"#Basic-Seaborn-XYPlot\" data-toc-modified-id=\"Basic-Seaborn-XYPlot-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Basic Seaborn XYPlot</a></span></li></ul></li><li><span><a href=\"#TODO:-Anomaly-Detection\" data-toc-modified-id=\"TODO:-Anomaly-Detection-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>TODO: Anomaly Detection</a></span></li><li><span><a href=\"#Data-Model\" data-toc-modified-id=\"Data-Model-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Data Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-Functionality\" data-toc-modified-id=\"Basic-Functionality-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Basic Functionality</a></span></li><li><span><a href=\"#Feature-selection-and-data-training-split\" data-toc-modified-id=\"Feature-selection-and-data-training-split-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Feature selection and data training split</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO-Select-variables-here\" data-toc-modified-id=\"TODO-Select-variables-here-4.4.2.1\"><span class=\"toc-item-num\">4.4.2.1&nbsp;&nbsp;</span>TODO Select variables here</a></span></li><li><span><a href=\"#TODO-Preliminary-Checks\" data-toc-modified-id=\"TODO-Preliminary-Checks-4.4.2.2\"><span class=\"toc-item-num\">4.4.2.2&nbsp;&nbsp;</span>TODO Preliminary Checks</a></span></li></ul></li><li><span><a href=\"#TODO-Naive-Linear-Regression\" data-toc-modified-id=\"TODO-Naive-Linear-Regression-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>TODO Naive Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ordinary-Linear-Regression\" data-toc-modified-id=\"Ordinary-Linear-Regression-4.4.3.1\"><span class=\"toc-item-num\">4.4.3.1&nbsp;&nbsp;</span>Ordinary Linear Regression</a></span></li><li><span><a href=\"#Ordinary-Linear-Regression-with-differentiation\" data-toc-modified-id=\"Ordinary-Linear-Regression-with-differentiation-4.4.3.2\"><span class=\"toc-item-num\">4.4.3.2&nbsp;&nbsp;</span>Ordinary Linear Regression with differentiation</a></span></li></ul></li><li><span><a href=\"#TODO-ARIMA(X)-model\" data-toc-modified-id=\"TODO-ARIMA(X)-model-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>TODO ARIMA(X) model</a></span></li></ul></li></ul></li><li><span><a href=\"#R-Framework\" data-toc-modified-id=\"R-Framework-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>R Framework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialise-environment\" data-toc-modified-id=\"Initialise-environment-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Initialise environment</a></span></li><li><span><a href=\"#Install-dependencies\" data-toc-modified-id=\"Install-dependencies-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Install dependencies</a></span></li><li><span><a href=\"#Load-in-R-libraries\" data-toc-modified-id=\"Load-in-R-libraries-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Load in R libraries</a></span></li><li><span><a href=\"#Export-Data-to-R-Dataframe\" data-toc-modified-id=\"Export-Data-to-R-Dataframe-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Export Data to R Dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Renaming-and-timestamp-reading\" data-toc-modified-id=\"Renaming-and-timestamp-reading-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Renaming and timestamp reading</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pairs-Plot\" data-toc-modified-id=\"Pairs-Plot-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Pairs Plot</a></span></li><li><span><a href=\"#Coplot\" data-toc-modified-id=\"Coplot-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>Coplot</a></span></li></ul></li><li><span><a href=\"#Model-Iterations\" data-toc-modified-id=\"Model-Iterations-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Model Iterations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Diagnostics\" data-toc-modified-id=\"Model-Diagnostics-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>Model Diagnostics</a></span></li><li><span><a href=\"#Model-Fit-Plot\" data-toc-modified-id=\"Model-Fit-Plot-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>Model Fit Plot</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Traditional-R\" data-toc-modified-id=\"Using-Traditional-R-5.6.2.1\"><span class=\"toc-item-num\">5.6.2.1&nbsp;&nbsp;</span>Using Traditional R</a></span></li><li><span><a href=\"#Using-interactive-Plot\" data-toc-modified-id=\"Using-interactive-Plot-5.6.2.2\"><span class=\"toc-item-num\">5.6.2.2&nbsp;&nbsp;</span>Using interactive Plot</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "357px",
    "width": "307px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "48px",
    "left": "552px",
    "top": "705.497px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
